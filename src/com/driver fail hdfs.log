Launch Command: "/usr/jdk64/jdk1.7.0_45/bin/java" "-cp" "/home/platuser/spark-1.6.2-bin-kerberos/conf/:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-assembly-1.6.2-hadoop2.4.1.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-rdbms-3.2.9.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-core-3.2.10.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/:/etc/hadoop/conf/" "-Xms1024M" "-Xmx1024M" "-Dspark.jars=file:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-examples-1.6.2-hadoop2.4.1.jar" "-Dspark.hadoop.security.kerberos.renewInterval=21600000" "-Dspark.hadoop.security.kerberos.principal=platuser/_HOST@OTOCYON.COM" "-Dspark.submit.deployMode=cluster" "-Dspark.hadoop.security.kerberos.keytab=/home/platuser/platuser.keytab" "-Dspark.master=spark://10.2.177.211:7077" "-Dspark.app.name=org.apache.spark.examples.HdfsTest" "-Dspark.hadoop.security.token.name=spark.token" "-Dspark.driver.supervise=true" "-Dspark.hadoop.security.authentication=kerberos" "-XX:MaxPermSize=256m" "org.apache.spark.deploy.worker.DriverWrapper" "spark://Worker@10.2.177.211:7078" "/var/run/spark/work/driver-20161025114811-0001/spark-examples-1.6.2-hadoop2.4.1.jar" "org.apache.spark.examples.HdfsTest" "/user/platuser/test2.txt"
========================================

16/10/25 11:48:12 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)], about=, type=DEFAULT, always=false, sampleName=Ops)
16/10/25 11:48:12 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)], about=, type=DEFAULT, always=false, sampleName=Ops)
16/10/25 11:48:12 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[GetGroups], about=, type=DEFAULT, always=false, sampleName=Ops)
16/10/25 11:48:12 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
16/10/25 11:48:12 DEBUG Groups:  Creating new Groups object
16/10/25 11:48:12 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
16/10/25 11:48:12 DEBUG NativeCodeLoader: Loaded the native-hadoop library
16/10/25 11:48:12 DEBUG JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
16/10/25 11:48:12 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
16/10/25 11:48:12 DEBUG Shell: setsid exited with exit code 0
16/10/25 11:48:12 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
16/10/25 11:48:12 DEBUG UserGroupInformation: hadoop login
16/10/25 11:48:12 DEBUG UserGroupInformation: hadoop login commit
16/10/25 11:48:12 DEBUG UserGroupInformation: using kerberos user:platuser/10.2.177.208@OTOCYON.COM
16/10/25 11:48:12 DEBUG UserGroupInformation: UGI loginUser:platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)
16/10/25 11:48:12 INFO SecurityManager: Changing view acls to: platuser
16/10/25 11:48:12 INFO SecurityManager: Changing modify acls to: platuser
16/10/25 11:48:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(platuser); users with modify permissions: Set(platuser)
16/10/25 11:48:13 DEBUG UserGroupInformation: Found tgt Ticket (hex) =
0000: 61 82 01 63 30 82 01 5F   A0 03 02 01 05 A1 0D 1B  a..c0.._........
0010: 0B 4F 54 4F 43 59 4F 4E   2E 43 4F 4D A2 20 30 1E  .OTOCYON.COM. 0.
0020: A0 03 02 01 02 A1 17 30   15 1B 06 6B 72 62 74 67  .......0...krbtg
0030: 74 1B 0B 4F 54 4F 43 59   4F 4E 2E 43 4F 4D A3 82  t..OTOCYON.COM..
0040: 01 25 30 82 01 21 A0 03   02 01 12 A1 03 02 01 01  .%0..!..........
0050: A2 82 01 13 04 82 01 0F   79 D1 21 7D 6E 71 57 A8  ........y.!.nqW.
0060: CF 3D CB 7C 21 E4 F8 C3   18 20 76 81 BF F9 4F 3C  .=..!.... v...O<
0070: 35 9F DD 5C 86 F5 60 06   B1 50 1F BA 6C 3F 23 A2  5..\..`..P..l?#.
0080: 92 98 60 3F 08 F4 13 63   8B 50 E8 B6 19 25 98 A5  ..`?...c.P...%..
0090: 38 A3 DA 54 70 66 C6 AF   F5 4E 49 DB B5 DD 78 6B  8..Tpf...NI...xk
00A0: 2A D9 EE 0E 58 F6 EF 12   DF F2 54 A6 90 C1 10 D5  *...X.....T.....
00B0: 63 1D 79 51 FF DA 00 76   F9 82 15 80 52 64 D8 59  c.yQ...v....Rd.Y
00C0: 2F 38 35 A0 03 CC 36 FA   90 56 73 0F A0 8B CA 10  /85...6..Vs.....
00D0: 4B F2 24 29 A9 62 B7 11   8A BD C9 D1 C7 FE 58 C4  K.$).b........X.
00E0: 82 0B E1 59 77 AE D0 20   6C 14 2A F2 26 C1 70 B2  ...Yw.. l.*.&.p.
00F0: C7 47 76 49 7E 02 9A 2E   00 94 C8 F2 CF 07 DF 63  .GvI...........c
0100: 77 E1 68 9A F0 14 60 C7   D5 E0 D8 54 5F 8E 14 55  w.h...`....T_..U
0110: 4E 3E BA 11 8D C7 BC C1   E3 57 46 AF AB 38 E4 C6  N>.......WF..8..
0120: 45 31 B4 5C A1 E8 6C 63   DD C3 83 3D D4 96 63 A6  E1.\..lc...=..c.
0130: 65 5C AF 52 5E 6A 9E 81   F8 47 59 56 9C 75 6B 8C  e\.R^j...GYV.uk.
0140: 13 3E 86 3C 62 1A F1 21   2E 4B 38 DE B1 9A CB 83  .>.<b..!.K8.....
0150: BF B5 2B 34 B9 04 E3 64   94 26 F2 55 D3 FF 92 3B  ..+4...d.&.U...;
0160: CA BB 93 9D F3 3E B4                               .....>.

Client Principal = platuser/10.2.177.208@OTOCYON.COM
Server Principal = krbtgt/OTOCYON.COM@OTOCYON.COM
Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)=
0000: E8 8D 26 1C 4B EE 88 CB   77 DE 5F FF 22 10 CD 20  ..&.K...w._."..
0010: 9D BE 4E 74 BC 4E 40 23   20 47 3D 26 D1 ED 00 BE  ..Nt.N@# G=&....


Forwardable Ticket true
Forwarded Ticket false
Proxiable Ticket false
Proxy Ticket false
Postdated Ticket false
Renewable Ticket true
Initial Ticket true
Auth Time = Mon Oct 24 16:37:24 CST 2016
Start Time = Mon Oct 24 16:37:24 CST 2016
End Time = Tue Oct 25 16:37:24 CST 2016
Renew Till = Thu Oct 27 16:37:24 CST 2016
Client Addresses  Null
16/10/25 11:48:13 DEBUG UserGroupInformation: Current time is 1477367293006
16/10/25 11:48:13 DEBUG UserGroupInformation: Next refresh is 1477367364000
16/10/25 11:48:13 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:48:13 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:48:13 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:48:13 DEBUG SecurityManager: SSLConfiguration for file server: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/25 11:48:13 DEBUG SecurityManager: SSLConfiguration for Akka: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/25 11:48:13 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
16/10/25 11:48:13 DEBUG PlatformDependent0: java.nio.Buffer.address: available
16/10/25 11:48:13 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
16/10/25 11:48:13 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
16/10/25 11:48:13 DEBUG PlatformDependent0: java.nio.Bits.unaligned: true
16/10/25 11:48:13 DEBUG PlatformDependent: Java version: 7
16/10/25 11:48:13 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
16/10/25 11:48:13 DEBUG PlatformDependent: sun.misc.Unsafe: available
16/10/25 11:48:13 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
16/10/25 11:48:13 DEBUG PlatformDependent: Javassist: unavailable
16/10/25 11:48:13 DEBUG PlatformDependent: You don't have Javassist in your class path or you don't have enough permission to load dynamically generated classes.  Please check the configuration for better performance.
16/10/25 11:48:13 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
16/10/25 11:48:13 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
16/10/25 11:48:13 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
16/10/25 11:48:13 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
16/10/25 11:48:13 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
16/10/25 11:48:13 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
16/10/25 11:48:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
16/10/25 11:48:13 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0x34054ae65ceb8753 (took 6 ms)
16/10/25 11:48:13 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
16/10/25 11:48:13 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
16/10/25 11:48:13 DEBUG NetUtil: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)
16/10/25 11:48:13 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
16/10/25 11:48:13 DEBUG TransportServer: Shuffle server started on port :41869
16/10/25 11:48:13 INFO Utils: Successfully started service 'Driver' on port 41869.
16/10/25 11:48:13 INFO WorkerWatcher: Connecting to worker spark://Worker@10.2.177.211:7078
16/10/25 11:48:13 DEBUG TransportClientFactory: Creating new connection to /10.2.177.211:7078
16/10/25 11:48:13 DEBUG ResourceLeakDetector: -Dio.netty.leakDetectionLevel: simple
16/10/25 11:48:13 DEBUG TransportClientFactory: Connection to /10.2.177.211:7078 successful, running bootstraps...
16/10/25 11:48:13 DEBUG TransportClientFactory: Successfully created connection to /10.2.177.211:7078 after 60 ms (0 ms spent in bootstraps)
16/10/25 11:48:13 DEBUG Recycler: -Dio.netty.recycler.maxCapacity.default: 262144
16/10/25 11:48:13 INFO SparkContext: Running Spark version 1.6.2
16/10/25 11:48:13 INFO SecurityManager: Changing view acls to: platuser
16/10/25 11:48:13 INFO SecurityManager: Changing modify acls to: platuser
16/10/25 11:48:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(platuser); users with modify permissions: Set(platuser)
16/10/25 11:48:13 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:48:13 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:48:13 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:48:13 DEBUG SecurityManager: SSLConfiguration for file server: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/25 11:48:13 DEBUG SecurityManager: SSLConfiguration for Akka: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/25 11:48:13 DEBUG TransportServer: Shuffle server started on port :45612
16/10/25 11:48:13 INFO Utils: Successfully started service 'sparkDriver' on port 45612.
16/10/25 11:48:13 DEBUG AkkaUtils: In createActorSystem, requireCookie is: off
16/10/25 11:48:14 INFO Slf4jLogger: Slf4jLogger started
16/10/25 11:48:14 INFO Remoting: Starting remoting
16/10/25 11:48:14 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.2.177.211:37813]
16/10/25 11:48:14 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 37813.
16/10/25 11:48:14 DEBUG SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
16/10/25 11:48:14 INFO SparkEnv: Registering MapOutputTracker
16/10/25 11:48:14 INFO SparkEnv: Registering BlockManagerMaster
16/10/25 11:48:14 INFO DiskBlockManager: Created local directory at /data_b/spark/blockmgr-6605eba8-ee20-4baa-8717-9b42481a1899
16/10/25 11:48:14 INFO MemoryStore: MemoryStore started with capacity 511.5 MB
16/10/25 11:48:14 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/25 11:48:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/25 11:48:14 INFO SparkUI: Started SparkUI at http://10.2.177.211:4040
16/10/25 11:48:14 DEBUG SparkHadoopUtil: ==getAuthenticationType==kerberos==
16/10/25 11:48:15 DEBUG : address: spark177211/10.2.177.211 isLoopbackAddress: false, with host 10.2.177.211 spark177211
16/10/25 11:48:15 DEBUG NativeLibraryLoader: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
16/10/25 11:48:15 DEBUG NativeLibraryLoader: -Dio.netty.netty.workdir: /tmp (io.netty.tmpdir)
16/10/25 11:48:15 DEBUG SparkHadoopUtil: ==localFS==org.apache.hadoop.fs.LocalFileSystem@187d503d==
16/10/25 11:48:15 DEBUG SparkHadoopUtil: ==tokenFile==file:/home/platuser/spark.token==
16/10/25 11:48:15 DEBUG SparkHadoopUtil: ==currentUser==platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)==
16/10/25 11:48:15 DEBUG BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
16/10/25 11:48:15 DEBUG BlockReaderLocal: dfs.client.read.shortcircuit = true
16/10/25 11:48:15 DEBUG BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
16/10/25 11:48:15 DEBUG BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
16/10/25 11:48:15 DEBUG RetryUtils: multipleLinearRandomRetry = null
16/10/25 11:48:15 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@7f351301
16/10/25 11:48:15 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@68222d20
16/10/25 11:48:15 DEBUG DomainSocketWatcher: org.apache.hadoop.net.unix.DomainSocketWatcher$1@51dd475f: starting with interruptCheckPeriodMs = 60000
16/10/25 11:48:15 DEBUG BlockReaderLocal: The short-circuit local reads feature is enabled.
16/10/25 11:48:15 DEBUG SparkHadoopUtil: ==fs==DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-784315784_1, ugi=platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)]]==
16/10/25 11:48:15 DEBUG Client: The ping interval is 60000 ms.
16/10/25 11:48:15 DEBUG Client: Connecting to master177214.otocyon.com/10.2.177.214:8020
16/10/25 11:48:15 DEBUG UserGroupInformation: PrivilegedAction as:platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:711)
16/10/25 11:48:15 DEBUG SaslRpcClient: Sending sasl message state: NEGOTIATE

16/10/25 11:48:15 DEBUG SaslRpcClient: Received SASL message state: NEGOTIATE
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
  challenge: "realm=\"default\",nonce=\"iisiainknXSd7HZoCX8RZVtRpsvePGxzVot2meEQ\",qop=\"auth\",charset=utf-8,algorithm=md5-sess"
}
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "master177214.otocyon.com"
}

16/10/25 11:48:15 DEBUG SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
16/10/25 11:48:15 DEBUG SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=, serverPrincipal=dfs.namenode.kerberos.principal)
16/10/25 11:48:15 DEBUG SaslRpcClient: getting serverKey: dfs.namenode.kerberos.principal conf value: hdfs/_HOST@OTOCYON.COM principal: hdfs/master177214.otocyon.com@OTOCYON.COM
16/10/25 11:48:15 DEBUG SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB is hdfs/master177214.otocyon.com@OTOCYON.COM
16/10/25 11:48:15 DEBUG SaslRpcClient: Creating SASL GSSAPI(KERBEROS)  client to authenticate to service at master177214.otocyon.com
16/10/25 11:48:15 DEBUG SaslRpcClient: Use KERBEROS authentication for protocol ClientNamenodeProtocolPB
16/10/25 11:48:15 DEBUG SaslRpcClient: Sending sasl message state: INITIATE
token: "`\202\002\200\006\t*\206H\206\367\022\001\002\002\001\000n\202\002o0\202\002k\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001ra\202\001n0\202\001j\240\003\002\001\005\241\r\033\vOTOCYON.COM\242+0)\240\003\002\001\000\241\"0 \033\004hdfs\033\030master177214.otocyon.com\243\202\001%0\202\001!\240\003\002\001\022\241\003\002\001\002\242\202\001\023\004\202\001\017\n\220d\017\325\224\035%\316\232\320x\267\250\320\255\017\316\\\275\204\324\'\347\343w\3252\200\325g\024\364\003q\326\213\301z\313\036\253\246Y\351\377BS\033\264\306\a\n\021\341\204\254\222\374\023\233\222O\033\312\'\027\333!)\305\364\3336E*\031\024\200\033\220\vS:\340:aIyz\n\"/\357\v\223\303\241c\212\033;\022\0252k`>\a\027\244\206Q\346\353\027\256\213\361Y#\245\213\rb\343\353\3062\275\036<\370\331\344\272\346\252.\245\271\210\311\242\264\366s\367\t\036\"c\216\246m\255\344\205\2431\v\251\223\206@\314\222\311c\362D\353XN\030\303Y\240K\262y\001o\032V2Qo\236;\376j\224\004\305B\0166\005e\233\224\342k!\367\251qJ%T\202\273\202&V\225\251\375Q \214\031\300\246d*\266Q`\261\337\361\244?L,\202\225\225\035C\205\3659\033\236\311\365.v\270\220\247\033+\340\371|\356\253\b\222\335={&\001\244\201\3370\201\334\240\003\002\001\022\242\201\324\004\201\321\"\033\336z\252{B\021\357r\247\233\031\362\334S>\363,\314\000\312\257V\301.\344?\371Z\313\237\355\354\036\306A\001\2506\241C/\243\204\264^)P\360b\037\211\315\200u\276\326\226\262b\232\006\217\002;\314\204\\\'`\320\371\245PXA}\022\277,\367dW\375\205\021\312U\331\371\246O\261n\364\254\243\307d@\242T\330j$\326k\373S\017\323\3138!Y\322\003\205\0173\0057\256\300:%T\'lA\326\026\235Zm8\352\024m\204\027\017\273\030\fv\036\212\255\331\276<i\303\356\202\331\234\321f\370#79\023\220\251\n\362\017\rO\236EF\3525#\021Q8\275\375\367q\262b\2771\236\206@\304x\367*$\335\027\246^\267\353\203\343Q"
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "master177214.otocyon.com"
}

16/10/25 11:48:15 DEBUG SaslRpcClient: Received SASL message state: CHALLENGE
token: "`j\006\t*\206H\206\367\022\001\002\002\002\000o[0Y\240\003\002\001\005\241\003\002\001\017\242M0K\240\003\002\001\022\242D\004BW\035G\255\220D\321\246.\006\264m\016\a#4\362\244d\326\"\373\016\250\242\t\224+\230#\346$H\317\326^\np\222]Y\024\230\363\234L\305\242\374\204\355q\233\020\000\302\374\354\326\366\241$\353\241\277"

16/10/25 11:48:15 DEBUG SaslRpcClient: Sending sasl message state: RESPONSE
token: ""

16/10/25 11:48:15 DEBUG SaslRpcClient: Received SASL message state: CHALLENGE
token: "\005\004\001\377\000\f\000\000\000\000\000\000 8\t\357\001\001\000\000\342\2234n\377\255\207Y)\335?\330"

16/10/25 11:48:15 DEBUG SaslRpcClient: Sending sasl message state: RESPONSE
token: "\005\004\000\377\000\f\000\000\000\000\000\000,\3131\324\001\001\000\000}@\345 \244\216\243\273sKeF"

16/10/25 11:48:15 DEBUG SaslRpcClient: Received SASL message state: SUCCESS

16/10/25 11:48:15 DEBUG Client: Negotiated QOP is :auth
16/10/25 11:48:15 DEBUG Client: IPC Client (1724774491) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: starting, having connections 1
16/10/25 11:48:15 DEBUG Client: IPC Client (1724774491) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM sending #0
16/10/25 11:48:15 DEBUG Client: IPC Client (1724774491) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM got value #0
16/10/25 11:48:15 DEBUG ProtobufRpcEngine: Call: getDelegationToken took 210ms
16/10/25 11:48:15 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 16277 for platuser on 10.2.177.214:8020
16/10/25 11:48:15 DEBUG SparkHadoopUtil: ==token==Kind: HDFS_DELEGATION_TOKEN, Service: 10.2.177.214:8020, Ident: (HDFS_DELEGATION_TOKEN token 16277 for platuser)==
16/10/25 11:48:15 DEBUG NativeIO: Initialized cache for IDs to User/Group mapping with a  cache timeout of 14400 seconds.
16/10/25 11:48:15 DEBUG SparkHadoopUtil: ==cred==org.apache.hadoop.security.Credentials@1249a33f==
16/10/25 11:48:15 INFO SparkHadoopUtil: Stored Hadoop delegation token for user platuser to file file:/home/platuser/spark.token
16/10/25 11:48:15 INFO HttpFileServer: HTTP File server directory is /data_b/spark/spark-45e58a5f-e7dd-475e-a135-8b86b3c535b6/httpd-cda1f7da-6da7-4695-aeee-094ba4da43a3
16/10/25 11:48:15 INFO HttpServer: Starting HTTP Server
16/10/25 11:48:15 DEBUG HttpServer: HttpServer is not using security
16/10/25 11:48:16 INFO Utils: Successfully started service 'HTTP file server' on port 50817.
16/10/25 11:48:16 DEBUG HttpFileServer: HTTP file server started at: http://10.2.177.211:50817
16/10/25 11:48:16 INFO Utils: Copying /home/platuser/spark.token to /data_b/spark/spark-45e58a5f-e7dd-475e-a135-8b86b3c535b6/userFiles-2c412608-baa0-4609-a282-c2eda5813d9a/spark.token
16/10/25 11:48:16 INFO SparkContext: Added file file:/home/platuser/spark.token at http://10.2.177.211:50817/files/spark.token with timestamp 1477367296033
16/10/25 11:48:16 DEBUG SparkHadoopUtil: ==localFS==org.apache.hadoop.fs.LocalFileSystem@187d503d==
16/10/25 11:48:16 DEBUG SparkHadoopUtil: ==tokenFile==file:/home/platuser/spark.token==
16/10/25 11:48:16 DEBUG SparkHadoopUtil: ==currentUser==platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)==
16/10/25 11:48:16 DEBUG SparkHadoopUtil: ==fs==DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-784315784_1, ugi=platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)]]==
16/10/25 11:48:16 DEBUG Client: IPC Client (1724774491) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM sending #1
16/10/25 11:48:16 DEBUG Client: IPC Client (1724774491) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM got value #1
16/10/25 11:48:16 DEBUG ProtobufRpcEngine: Call: getDelegationToken took 3ms
16/10/25 11:48:16 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 16278 for platuser on 10.2.177.214:8020
16/10/25 11:48:16 DEBUG SparkHadoopUtil: ==token==Kind: HDFS_DELEGATION_TOKEN, Service: 10.2.177.214:8020, Ident: (HDFS_DELEGATION_TOKEN token 16278 for platuser)==
16/10/25 11:48:16 DEBUG SparkHadoopUtil: ==cred==org.apache.hadoop.security.Credentials@7761588b==
16/10/25 11:48:16 INFO SparkHadoopUtil: Stored Hadoop delegation token for user platuser to file file:/home/platuser/spark.token
16/10/25 11:48:16 DEBUG SparkHadoopUtil: ==initDelegationToken().toString==file:/home/platuser/spark.token==
16/10/25 11:48:16 INFO SparkContext: Added JAR file:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-examples-1.6.2-hadoop2.4.1.jar at http://10.2.177.211:50817/jars/spark-examples-1.6.2-hadoop2.4.1.jar with timestamp 1477367296246
16/10/25 11:48:16 INFO AppClient$ClientEndpoint: Connecting to master spark://10.2.177.211:7077...
16/10/25 11:48:16 DEBUG TransportClientFactory: Creating new connection to /10.2.177.211:7077
16/10/25 11:48:16 DEBUG TransportClientFactory: Connection to /10.2.177.211:7077 successful, running bootstraps...
16/10/25 11:48:16 DEBUG TransportClientFactory: Successfully created connection to /10.2.177.211:7077 after 2 ms (0 ms spent in bootstraps)
16/10/25 11:48:16 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20161025114816-0001
16/10/25 11:48:16 INFO AppClient$ClientEndpoint: Executor added: app-20161025114816-0001/0 on worker-20161025113448-10.2.177.211-7078 (10.2.177.211:7078) with 1 cores
16/10/25 11:48:16 INFO SparkDeploySchedulerBackend: Granted executor ID app-20161025114816-0001/0 on hostPort 10.2.177.211:7078 with 1 cores, 1024.0 MB RAM
16/10/25 11:48:16 DEBUG TransportServer: Shuffle server started on port :55243
16/10/25 11:48:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55243.
16/10/25 11:48:16 INFO NettyBlockTransferService: Server created on 55243
16/10/25 11:48:16 INFO BlockManagerMaster: Trying to register BlockManager
16/10/25 11:48:16 INFO BlockManagerMasterEndpoint: Registering block manager 10.2.177.211:55243 with 511.5 MB RAM, BlockManagerId(driver, 10.2.177.211, 55243)
16/10/25 11:48:16 INFO BlockManagerMaster: Registered BlockManager
16/10/25 11:48:16 INFO AppClient$ClientEndpoint: Executor updated: app-20161025114816-0001/0 is now RUNNING
16/10/25 11:48:16 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/10/25 11:48:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 223.0 KB, free 223.0 KB)
16/10/25 11:48:17 DEBUG BlockManager: Put block broadcast_0 locally took  169 ms
16/10/25 11:48:17 DEBUG BlockManager: Putting block broadcast_0 without replication took  170 ms
16/10/25 11:48:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.8 KB, free 242.9 KB)
16/10/25 11:48:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.177.211:55243 (size: 19.8 KB, free: 511.5 MB)
16/10/25 11:48:17 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
16/10/25 11:48:17 DEBUG BlockManager: Told master about block broadcast_0_piece0
16/10/25 11:48:17 DEBUG BlockManager: Put block broadcast_0_piece0 locally took  7 ms
16/10/25 11:48:17 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took  7 ms
16/10/25 11:48:17 INFO SparkContext: Created broadcast 0 from textFile at HdfsTest.scala:34
16/10/25 11:48:17 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33) +++
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared fields: 2
16/10/25 11:48:17 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.serialVersionUID
16/10/25 11:48:17 DEBUG ClosureCleaner:      private final org.apache.spark.SparkContext$$anonfun$hadoopFile$1 org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.$outer
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared methods: 2
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final void org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(org.apache.hadoop.mapred.JobConf)
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(java.lang.Object)
16/10/25 11:48:17 DEBUG ClosureCleaner:  + inner classes: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer classes: 2
16/10/25 11:48:17 DEBUG ClosureCleaner:      org.apache.spark.SparkContext$$anonfun$hadoopFile$1
16/10/25 11:48:17 DEBUG ClosureCleaner:      org.apache.spark.SparkContext
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer objects: 2
16/10/25 11:48:17 DEBUG ClosureCleaner:      <function0>
16/10/25 11:48:17 DEBUG ClosureCleaner:      org.apache.spark.SparkContext@4ba24c26
16/10/25 11:48:17 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/25 11:48:17 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
16/10/25 11:48:17 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext,Set())
16/10/25 11:48:17 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext$$anonfun$hadoopFile$1,Set(path$6))
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outermost object is not a closure, so do not clone it: (class org.apache.spark.SparkContext,org.apache.spark.SparkContext@4ba24c26)
16/10/25 11:48:17 DEBUG ClosureCleaner:  + cloning the object <function0> of class org.apache.spark.SparkContext$$anonfun$hadoopFile$1
16/10/25 11:48:17 DEBUG ClosureCleaner:  + cleaning cloned closure <function0> recursively (org.apache.spark.SparkContext$$anonfun$hadoopFile$1)
16/10/25 11:48:17 DEBUG ClosureCleaner: +++ Cleaning closure <function0> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1) +++
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared fields: 7
16/10/25 11:48:17 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$hadoopFile$1.serialVersionUID
16/10/25 11:48:17 DEBUG ClosureCleaner:      private final org.apache.spark.SparkContext org.apache.spark.SparkContext$$anonfun$hadoopFile$1.$outer
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final java.lang.String org.apache.spark.SparkContext$$anonfun$hadoopFile$1.path$6
16/10/25 11:48:17 DEBUG ClosureCleaner:      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.inputFormatClass$1
16/10/25 11:48:17 DEBUG ClosureCleaner:      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.keyClass$1
16/10/25 11:48:17 DEBUG ClosureCleaner:      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.valueClass$1
16/10/25 11:48:17 DEBUG ClosureCleaner:      private final int org.apache.spark.SparkContext$$anonfun$hadoopFile$1.minPartitions$3
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared methods: 2
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final org.apache.spark.rdd.HadoopRDD org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply()
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply()
16/10/25 11:48:17 DEBUG ClosureCleaner:  + inner classes: 1
16/10/25 11:48:17 DEBUG ClosureCleaner:      org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer classes: 1
16/10/25 11:48:17 DEBUG ClosureCleaner:      org.apache.spark.SparkContext
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer objects: 1
16/10/25 11:48:17 DEBUG ClosureCleaner:      org.apache.spark.SparkContext@4ba24c26
16/10/25 11:48:17 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
16/10/25 11:48:17 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext,Set())
16/10/25 11:48:17 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext$$anonfun$hadoopFile$1,Set(path$6))
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outermost object is not a closure, so do not clone it: (class org.apache.spark.SparkContext,org.apache.spark.SparkContext@4ba24c26)
16/10/25 11:48:17 DEBUG ClosureCleaner:  + the starting closure doesn't actually need org.apache.spark.SparkContext@4ba24c26, so we null it out
16/10/25 11:48:17 DEBUG ClosureCleaner:  +++ closure <function0> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1) is now cleaned +++
16/10/25 11:48:17 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33) is now cleaned +++
16/10/25 11:48:17 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9) +++
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared fields: 1
16/10/25 11:48:17 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9.serialVersionUID
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared methods: 2
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final java.lang.String org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9.apply(scala.Tuple2)
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9.apply(java.lang.Object)
16/10/25 11:48:17 DEBUG ClosureCleaner:  + inner classes: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer classes: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer objects: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/25 11:48:17 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/25 11:48:17 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9) is now cleaned +++
16/10/25 11:48:17 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.examples.HdfsTest$$anonfun$1) +++
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared fields: 1
16/10/25 11:48:17 DEBUG ClosureCleaner:      public static final long org.apache.spark.examples.HdfsTest$$anonfun$1.serialVersionUID
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared methods: 2
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final int org.apache.spark.examples.HdfsTest$$anonfun$1.apply(java.lang.String)
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.examples.HdfsTest$$anonfun$1.apply(java.lang.Object)
16/10/25 11:48:17 DEBUG ClosureCleaner:  + inner classes: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer classes: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer objects: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/25 11:48:17 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/25 11:48:17 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.examples.HdfsTest$$anonfun$1) is now cleaned +++
16/10/25 11:48:17 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1) +++
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared fields: 1
16/10/25 11:48:17 DEBUG ClosureCleaner:      public static final long org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.serialVersionUID
16/10/25 11:48:17 DEBUG ClosureCleaner:  + declared methods: 3
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(java.lang.Object)
16/10/25 11:48:17 DEBUG ClosureCleaner:      public final void org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(int)
16/10/25 11:48:17 DEBUG ClosureCleaner:      public void org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(int)
16/10/25 11:48:17 DEBUG ClosureCleaner:  + inner classes: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer classes: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + outer objects: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/25 11:48:17 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/25 11:48:17 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/25 11:48:17 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1) is now cleaned +++
16/10/25 11:48:17 DEBUG BlockManager: Getting local block broadcast_0
16/10/25 11:48:17 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(true, true, false, true, 1)
16/10/25 11:48:17 DEBUG BlockManager: Getting block broadcast_0 from memory
16/10/25 11:48:17 DEBUG HadoopRDD: SplitLocationInfo and other new Hadoop classes are unavailable. Using the older Hadoop location info code.
java.lang.ClassNotFoundException: org.apache.hadoop.mapred.InputSplitWithLocationInfo
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.rdd.HadoopRDD$SplitInfoReflections.<init>(HadoopRDD.scala:392)
	at org.apache.spark.rdd.HadoopRDD$.liftedTree1$1(HadoopRDD.scala:402)
	at org.apache.spark.rdd.HadoopRDD$.<init>(HadoopRDD.scala:401)
	at org.apache.spark.rdd.HadoopRDD$.<clinit>(HadoopRDD.scala)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:165)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:195)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1935)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:912)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:910)
	at org.apache.spark.examples.HdfsTest$$anonfun$main$1.apply$mcVI$sp(HdfsTest.scala:38)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.examples.HdfsTest$.main(HdfsTest.scala:36)
	at org.apache.spark.examples.HdfsTest.main(HdfsTest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)
16/10/25 11:48:17 DEBUG HadoopRDD: Creating new JobConf and caching it for later re-use
Exception in thread "main" java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)
Caused by: java.io.IOException: Can't get Master Kerberos principal for use as renewer
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:116)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:304)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1935)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:912)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:910)
	at org.apache.spark.examples.HdfsTest$$anonfun$main$1.apply$mcVI$sp(HdfsTest.scala:38)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.examples.HdfsTest$.main(HdfsTest.scala:36)
	at org.apache.spark.examples.HdfsTest.main(HdfsTest.scala)
	... 6 more
16/10/25 11:48:17 INFO SparkContext: Invoking stop() from shutdown hook
16/10/25 11:48:17 INFO SparkUI: Stopped Spark web UI at http://10.2.177.211:4040
16/10/25 11:48:17 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/10/25 11:48:17 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/10/25 11:48:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/25 11:48:17 INFO MemoryStore: MemoryStore cleared
16/10/25 11:48:17 INFO BlockManager: BlockManager stopped
16/10/25 11:48:17 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/25 11:48:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/25 11:48:17 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/10/25 11:48:17 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/10/25 11:48:17 INFO SparkContext: Successfully stopped SparkContext
16/10/25 11:48:17 INFO ShutdownHookManager: Shutdown hook called
16/10/25 11:48:17 INFO ShutdownHookManager: Deleting directory /data_b/spark/spark-45e58a5f-e7dd-475e-a135-8b86b3c535b6/httpd-cda1f7da-6da7-4695-aeee-094ba4da43a3
16/10/25 11:48:17 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
16/10/25 11:48:17 INFO ShutdownHookManager: Deleting directory /data_b/spark/spark-45e58a5f-e7dd-475e-a135-8b86b3c535b6
16/10/25 11:48:17 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@68222d20
16/10/25 11:48:17 DEBUG Client: removing client from cache: org.apache.hadoop.ipc.Client@68222d20
16/10/25 11:48:17 DEBUG Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@68222d20
16/10/25 11:48:17 DEBUG Client: Stopping client
16/10/25 11:48:17 DEBUG Client: IPC Client (1724774491) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: closed
16/10/25 11:48:17 DEBUG Client: IPC Client (1724774491) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: stopped, remaining connections 0