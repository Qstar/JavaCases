Launch Command: "/usr/jdk64/jdk1.7.0_45/bin/java" "-cp" "/home/platuser/spark-1.6.2-bin-kerberos/conf/:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-assembly-1.6.2-hadoop2.4.1.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-rdbms-3.2.9.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-core-3.2.10.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/:/etc/hadoop/conf/" "-Xms1024M" "-Xmx1024M" "-Dspark.jars=file:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-examples-1.6.2-hadoop2.4.1.jar" "-Dspark.hadoop.security.kerberos.renewInterval=21600000" "-Dspark.hadoop.security.kerberos.principal=platuser/_HOST@OTOCYON.COM" "-Dspark.submit.deployMode=cluster" "-Dspark.hadoop.security.kerberos.keytab=/home/platuser/platuser.keytab" "-Dspark.app.name=org.apache.spark.examples.SparkPi" "-Dspark.master=spark://10.2.177.211:7077" "-Dspark.hadoop.security.token.name=spark.token" "-Dspark.driver.supervise=true" "-Dspark.hadoop.security.authentication=kerberos" "-XX:MaxPermSize=256m" "org.apache.spark.deploy.worker.DriverWrapper" "spark://Worker@10.2.177.211:7078" "/var/run/spark/work/driver-20161025113453-0000/spark-examples-1.6.2-hadoop2.4.1.jar" "org.apache.spark.examples.SparkPi"
========================================

16/10/25 11:34:54 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)], about=, type=DEFAULT, always=false, sampleName=Ops)
16/10/25 11:34:54 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)], about=, type=DEFAULT, always=false, sampleName=Ops)
16/10/25 11:34:54 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[GetGroups], about=, type=DEFAULT, always=false, sampleName=Ops)
16/10/25 11:34:54 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
16/10/25 11:34:55 DEBUG Groups:  Creating new Groups object
16/10/25 11:34:55 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
16/10/25 11:34:55 DEBUG NativeCodeLoader: Loaded the native-hadoop library
16/10/25 11:34:55 DEBUG JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
16/10/25 11:34:55 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
16/10/25 11:34:55 DEBUG Shell: setsid exited with exit code 0
16/10/25 11:34:55 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
16/10/25 11:34:55 DEBUG UserGroupInformation: hadoop login
16/10/25 11:34:55 DEBUG UserGroupInformation: hadoop login commit
16/10/25 11:34:55 DEBUG UserGroupInformation: using kerberos user:platuser/10.2.177.208@OTOCYON.COM
16/10/25 11:34:55 DEBUG UserGroupInformation: UGI loginUser:platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)
16/10/25 11:34:55 INFO SecurityManager: Changing view acls to: platuser
16/10/25 11:34:55 INFO SecurityManager: Changing modify acls to: platuser
16/10/25 11:34:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(platuser); users with modify permissions: Set(platuser)
16/10/25 11:34:55 DEBUG UserGroupInformation: Found tgt Ticket (hex) =
0000: 61 82 01 63 30 82 01 5F   A0 03 02 01 05 A1 0D 1B  a..c0.._........
0010: 0B 4F 54 4F 43 59 4F 4E   2E 43 4F 4D A2 20 30 1E  .OTOCYON.COM. 0.
0020: A0 03 02 01 02 A1 17 30   15 1B 06 6B 72 62 74 67  .......0...krbtg
0030: 74 1B 0B 4F 54 4F 43 59   4F 4E 2E 43 4F 4D A3 82  t..OTOCYON.COM..
0040: 01 25 30 82 01 21 A0 03   02 01 12 A1 03 02 01 01  .%0..!..........
0050: A2 82 01 13 04 82 01 0F   79 D1 21 7D 6E 71 57 A8  ........y.!.nqW.
0060: CF 3D CB 7C 21 E4 F8 C3   18 20 76 81 BF F9 4F 3C  .=..!.... v...O<
0070: 35 9F DD 5C 86 F5 60 06   B1 50 1F BA 6C 3F 23 A2  5..\..`..P..l?#.
0080: 92 98 60 3F 08 F4 13 63   8B 50 E8 B6 19 25 98 A5  ..`?...c.P...%..
0090: 38 A3 DA 54 70 66 C6 AF   F5 4E 49 DB B5 DD 78 6B  8..Tpf...NI...xk
00A0: 2A D9 EE 0E 58 F6 EF 12   DF F2 54 A6 90 C1 10 D5  *...X.....T.....
00B0: 63 1D 79 51 FF DA 00 76   F9 82 15 80 52 64 D8 59  c.yQ...v....Rd.Y
00C0: 2F 38 35 A0 03 CC 36 FA   90 56 73 0F A0 8B CA 10  /85...6..Vs.....
00D0: 4B F2 24 29 A9 62 B7 11   8A BD C9 D1 C7 FE 58 C4  K.$).b........X.
00E0: 82 0B E1 59 77 AE D0 20   6C 14 2A F2 26 C1 70 B2  ...Yw.. l.*.&.p.
00F0: C7 47 76 49 7E 02 9A 2E   00 94 C8 F2 CF 07 DF 63  .GvI...........c
0100: 77 E1 68 9A F0 14 60 C7   D5 E0 D8 54 5F 8E 14 55  w.h...`....T_..U
0110: 4E 3E BA 11 8D C7 BC C1   E3 57 46 AF AB 38 E4 C6  N>.......WF..8..
0120: 45 31 B4 5C A1 E8 6C 63   DD C3 83 3D D4 96 63 A6  E1.\..lc...=..c.
0130: 65 5C AF 52 5E 6A 9E 81   F8 47 59 56 9C 75 6B 8C  e\.R^j...GYV.uk.
0140: 13 3E 86 3C 62 1A F1 21   2E 4B 38 DE B1 9A CB 83  .>.<b..!.K8.....
0150: BF B5 2B 34 B9 04 E3 64   94 26 F2 55 D3 FF 92 3B  ..+4...d.&.U...;
0160: CA BB 93 9D F3 3E B4                               .....>.

Client Principal = platuser/10.2.177.208@OTOCYON.COM
Server Principal = krbtgt/OTOCYON.COM@OTOCYON.COM
Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)=
0000: E8 8D 26 1C 4B EE 88 CB   77 DE 5F FF 22 10 CD 20  ..&.K...w._."..
0010: 9D BE 4E 74 BC 4E 40 23   20 47 3D 26 D1 ED 00 BE  ..Nt.N@# G=&....


Forwardable Ticket true
Forwarded Ticket false
Proxiable Ticket false
Proxy Ticket false
Postdated Ticket false
Renewable Ticket true
Initial Ticket true
Auth Time = Mon Oct 24 16:37:24 CST 2016
Start Time = Mon Oct 24 16:37:24 CST 2016
End Time = Tue Oct 25 16:37:24 CST 2016
Renew Till = Thu Oct 27 16:37:24 CST 2016
Client Addresses  Null
16/10/25 11:34:55 DEBUG UserGroupInformation: Current time is 1477366495492
16/10/25 11:34:55 DEBUG UserGroupInformation: Next refresh is 1477367364000
16/10/25 11:34:55 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:34:55 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:34:55 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:34:55 DEBUG SecurityManager: SSLConfiguration for file server: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/25 11:34:55 DEBUG SecurityManager: SSLConfiguration for Akka: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/25 11:34:55 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
16/10/25 11:34:55 DEBUG PlatformDependent0: java.nio.Buffer.address: available
16/10/25 11:34:55 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
16/10/25 11:34:55 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
16/10/25 11:34:55 DEBUG PlatformDependent0: java.nio.Bits.unaligned: true
16/10/25 11:34:55 DEBUG PlatformDependent: Java version: 7
16/10/25 11:34:55 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
16/10/25 11:34:55 DEBUG PlatformDependent: sun.misc.Unsafe: available
16/10/25 11:34:55 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
16/10/25 11:34:55 DEBUG PlatformDependent: Javassist: unavailable
16/10/25 11:34:55 DEBUG PlatformDependent: You don't have Javassist in your class path or you don't have enough permission to load dynamically generated classes.  Please check the configuration for better performance.
16/10/25 11:34:55 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
16/10/25 11:34:55 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
16/10/25 11:34:55 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
16/10/25 11:34:55 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
16/10/25 11:34:55 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
16/10/25 11:34:55 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
16/10/25 11:34:55 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
16/10/25 11:34:55 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0x0b0fb89c594e3c7f (took 6 ms)
16/10/25 11:34:56 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
16/10/25 11:34:56 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
16/10/25 11:34:56 DEBUG NetUtil: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)
16/10/25 11:34:56 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
16/10/25 11:34:56 DEBUG TransportServer: Shuffle server started on port :33731
16/10/25 11:34:56 INFO Utils: Successfully started service 'Driver' on port 33731.
16/10/25 11:34:56 INFO WorkerWatcher: Connecting to worker spark://Worker@10.2.177.211:7078
16/10/25 11:34:56 DEBUG TransportClientFactory: Creating new connection to /10.2.177.211:7078
16/10/25 11:34:56 DEBUG ResourceLeakDetector: -Dio.netty.leakDetectionLevel: simple
16/10/25 11:34:56 DEBUG TransportClientFactory: Connection to /10.2.177.211:7078 successful, running bootstraps...
16/10/25 11:34:56 DEBUG TransportClientFactory: Successfully created connection to /10.2.177.211:7078 after 64 ms (0 ms spent in bootstraps)
16/10/25 11:34:56 DEBUG Recycler: -Dio.netty.recycler.maxCapacity.default: 262144
16/10/25 11:34:56 INFO SparkContext: Running Spark version 1.6.2
16/10/25 11:34:56 INFO SecurityManager: Changing view acls to: platuser
16/10/25 11:34:56 INFO SecurityManager: Changing modify acls to: platuser
16/10/25 11:34:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(platuser); users with modify permissions: Set(platuser)
16/10/25 11:34:56 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:34:56 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:34:56 DEBUG SSLOptions: No SSL protocol specified
16/10/25 11:34:56 DEBUG SecurityManager: SSLConfiguration for file server: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/25 11:34:56 DEBUG SecurityManager: SSLConfiguration for Akka: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/25 11:34:56 DEBUG TransportServer: Shuffle server started on port :40030
16/10/25 11:34:56 INFO Utils: Successfully started service 'sparkDriver' on port 40030.
16/10/25 11:34:56 DEBUG AkkaUtils: In createActorSystem, requireCookie is: off
16/10/25 11:34:56 INFO Slf4jLogger: Slf4jLogger started
16/10/25 11:34:56 INFO Remoting: Starting remoting
16/10/25 11:34:56 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.2.177.211:54289]
16/10/25 11:34:56 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 54289.
16/10/25 11:34:56 DEBUG SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
16/10/25 11:34:57 INFO SparkEnv: Registering MapOutputTracker
16/10/25 11:34:57 INFO SparkEnv: Registering BlockManagerMaster
16/10/25 11:34:57 INFO DiskBlockManager: Created local directory at /data_b/spark/blockmgr-bd8a1709-4318-4e83-84b7-33b0e3a561eb
16/10/25 11:34:57 INFO MemoryStore: MemoryStore started with capacity 511.5 MB
16/10/25 11:34:57 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/25 11:34:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/25 11:34:57 INFO SparkUI: Started SparkUI at http://10.2.177.211:4040
16/10/25 11:34:57 DEBUG SparkHadoopUtil: ==getAuthenticationType==kerberos==
16/10/25 11:34:57 DEBUG : address: spark177211/10.2.177.211 isLoopbackAddress: false, with host 10.2.177.211 spark177211
16/10/25 11:34:57 DEBUG NativeLibraryLoader: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
16/10/25 11:34:57 DEBUG NativeLibraryLoader: -Dio.netty.netty.workdir: /tmp (io.netty.tmpdir)
16/10/25 11:34:57 DEBUG SparkHadoopUtil: ==localFS==org.apache.hadoop.fs.LocalFileSystem@24ae5291==
16/10/25 11:34:57 DEBUG SparkHadoopUtil: ==tokenFile==file:/home/platuser/spark.token==
16/10/25 11:34:57 DEBUG SparkHadoopUtil: ==currentUser==platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)==
16/10/25 11:34:57 DEBUG BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
16/10/25 11:34:57 DEBUG BlockReaderLocal: dfs.client.read.shortcircuit = true
16/10/25 11:34:57 DEBUG BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
16/10/25 11:34:57 DEBUG BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
16/10/25 11:34:57 DEBUG RetryUtils: multipleLinearRandomRetry = null
16/10/25 11:34:57 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@65a26081
16/10/25 11:34:57 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@67832d15
16/10/25 11:34:58 DEBUG DomainSocketWatcher: org.apache.hadoop.net.unix.DomainSocketWatcher$1@6e244773: starting with interruptCheckPeriodMs = 60000
16/10/25 11:34:58 DEBUG BlockReaderLocal: The short-circuit local reads feature is enabled.
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==fs==DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1513303678_1, ugi=platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)]]==
16/10/25 11:34:58 DEBUG Client: The ping interval is 60000 ms.
16/10/25 11:34:58 DEBUG Client: Connecting to master177214.otocyon.com/10.2.177.214:8020
16/10/25 11:34:58 DEBUG UserGroupInformation: PrivilegedAction as:platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:711)
16/10/25 11:34:58 DEBUG SaslRpcClient: Sending sasl message state: NEGOTIATE

16/10/25 11:34:58 DEBUG SaslRpcClient: Received SASL message state: NEGOTIATE
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
  challenge: "realm=\"default\",nonce=\"Es3qo8xoHZQqWVXHa1CKMn9aNNvQWTxuaJcQ/uhF\",qop=\"auth\",charset=utf-8,algorithm=md5-sess"
}
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "master177214.otocyon.com"
}

16/10/25 11:34:58 DEBUG SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
16/10/25 11:34:58 DEBUG SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=, serverPrincipal=dfs.namenode.kerberos.principal)
16/10/25 11:34:58 DEBUG SaslRpcClient: getting serverKey: dfs.namenode.kerberos.principal conf value: hdfs/_HOST@OTOCYON.COM principal: hdfs/master177214.otocyon.com@OTOCYON.COM
16/10/25 11:34:58 DEBUG SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB is hdfs/master177214.otocyon.com@OTOCYON.COM
16/10/25 11:34:58 DEBUG SaslRpcClient: Creating SASL GSSAPI(KERBEROS)  client to authenticate to service at master177214.otocyon.com
16/10/25 11:34:58 DEBUG SaslRpcClient: Use KERBEROS authentication for protocol ClientNamenodeProtocolPB
16/10/25 11:34:58 DEBUG SaslRpcClient: Sending sasl message state: INITIATE
token: "`\202\002\200\006\t*\206H\206\367\022\001\002\002\001\000n\202\002o0\202\002k\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001ra\202\001n0\202\001j\240\003\002\001\005\241\r\033\vOTOCYON.COM\242+0)\240\003\002\001\000\241\"0 \033\004hdfs\033\030master177214.otocyon.com\243\202\001%0\202\001!\240\003\002\001\022\241\003\002\001\002\242\202\001\023\004\202\001\017Je?{\003nq\024*4\"\200\2048\027\250\226\036\344=\v\030\203b\032\317\225\367\002\325!.\313\317%\320\034\t^\222)\213\244\t@\245\340{PD\351\304\371c9\245h\305\023\"B\000\237\001b\316w\261f\303\'y6\a\302\210\346\333mb3\263\tF\323\241\276]\323\247?\257\315C\232\325\276\001Z\207\271\247\235s\210\357\305\3223\3724p\337\337\356\266\f\215a\353\271c\'t\357\307\356\271\220\313Zc\361B8\276\333\336\001\354X9\331\317\035\006\244R#\340\272&\a{\f|Y\276\222\255\021t\217\020\006,\251C\271\220\020`DG\262\341a\330o\361\022\254\253\200\247\340\345b\204\244\235\354\256[\224\231U\311Z\244\276\262\322\027H\037\253%\376y\302\235\342@\276}^\364\035\000\032B\366\344\362\023^D\022\377\235\206\351\330\232%6h\323\f\204\344\313V+\030\314\371%:y0\247+JT\256l^\026%\333\025\25424\356\242V\244\201\3370\201\334\240\003\002\001\022\242\201\324\004\201\321\024\320\245\247y\317\"\020\215\267\006\025$\207k6\002\001\213\234%\022\333y\307^\031\267F\003\204$\025\355\303\351/\264\230tt\232\236\234\353{\302.bs \277\352\254\233\207\237\237\313\374\215\342\232\2434\206\255V\n\307\224\350`\314\025R\b\275\275\255\375\222\n\262\350\227\305w\365\026\212\273\347\373\336s\\\214\003Tk\232\357\247\363\330Dt\217$\r\246\342\245\255\003<2\217\215\b\212\313\035\234m\211{\266\364\234\3327\374=w#\277\365\036\345M\356\236dn\031\206IBh\211f\260\020\250\026,\360{\270Z\245\333o~v\360\267\225\034\213c\254u3[\335\336\254V\200}\034o\246\226#\276\3513d\271\024`z0=\220n\261\275\'\307JxQl\""
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "master177214.otocyon.com"
}

16/10/25 11:34:58 DEBUG SaslRpcClient: Received SASL message state: CHALLENGE
token: "`j\006\t*\206H\206\367\022\001\002\002\002\000o[0Y\240\003\002\001\005\241\003\002\001\017\242M0K\240\003\002\001\022\242D\004B\n\035HC7\220]4y\341\357\230\252\'c\366\\\v\250\003\374K\320\247\001\205\027\300:\213%{z \3503N\034\324}\312\236\332\016^\211X\243\240\335J\032Ci\201v\031S\234\2370nL\310\fu"

16/10/25 11:34:58 DEBUG SaslRpcClient: Sending sasl message state: RESPONSE
token: ""

16/10/25 11:34:58 DEBUG SaslRpcClient: Received SASL message state: CHALLENGE
token: "\005\004\001\377\000\f\000\000\000\000\000\000+\316\332w\001\001\000\000\315\022?>{g\253\313a^7&"

16/10/25 11:34:58 DEBUG SaslRpcClient: Sending sasl message state: RESPONSE
token: "\005\004\000\377\000\f\000\000\000\000\000\000\037e\253\304\001\001\000\000\035\a\210\203_0\341\255\342\361\314R"

16/10/25 11:34:58 DEBUG SaslRpcClient: Received SASL message state: SUCCESS

16/10/25 11:34:58 DEBUG Client: Negotiated QOP is :auth
16/10/25 11:34:58 DEBUG Client: IPC Client (1611843139) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: starting, having connections 1
16/10/25 11:34:58 DEBUG Client: IPC Client (1611843139) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM sending #0
16/10/25 11:34:58 DEBUG Client: IPC Client (1611843139) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM got value #0
16/10/25 11:34:58 DEBUG ProtobufRpcEngine: Call: getDelegationToken took 204ms
16/10/25 11:34:58 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 16272 for platuser on 10.2.177.214:8020
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==token==Kind: HDFS_DELEGATION_TOKEN, Service: 10.2.177.214:8020, Ident: (HDFS_DELEGATION_TOKEN token 16272 for platuser)==
16/10/25 11:34:58 DEBUG NativeIO: Initialized cache for IDs to User/Group mapping with a  cache timeout of 14400 seconds.
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==cred==org.apache.hadoop.security.Credentials@7ededd43==
16/10/25 11:34:58 INFO SparkHadoopUtil: Stored Hadoop delegation token for user platuser to file file:/home/platuser/spark.token
16/10/25 11:34:58 INFO HttpFileServer: HTTP File server directory is /data_b/spark/spark-96d63621-5512-413d-8426-583d7aa2278e/httpd-8e598449-c0d3-42ae-9bd6-7dccf888f91f
16/10/25 11:34:58 INFO HttpServer: Starting HTTP Server
16/10/25 11:34:58 DEBUG HttpServer: HttpServer is not using security
16/10/25 11:34:58 INFO Utils: Successfully started service 'HTTP file server' on port 53704.
16/10/25 11:34:58 DEBUG HttpFileServer: HTTP file server started at: http://10.2.177.211:53704
16/10/25 11:34:58 INFO Utils: Copying /home/platuser/spark.token to /data_b/spark/spark-96d63621-5512-413d-8426-583d7aa2278e/userFiles-35576cbb-4ac2-4c5c-befc-b6dfa3578966/spark.token
16/10/25 11:34:58 INFO SparkContext: Added file file:/home/platuser/spark.token at http://10.2.177.211:53704/files/spark.token with timestamp 1477366498609
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==localFS==org.apache.hadoop.fs.LocalFileSystem@24ae5291==
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==tokenFile==file:/home/platuser/spark.token==
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==currentUser==platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)==
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==fs==DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1513303678_1, ugi=platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)]]==
16/10/25 11:34:58 DEBUG Client: IPC Client (1611843139) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM sending #1
16/10/25 11:34:58 DEBUG Client: IPC Client (1611843139) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM got value #1
16/10/25 11:34:58 DEBUG ProtobufRpcEngine: Call: getDelegationToken took 5ms
16/10/25 11:34:58 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 16273 for platuser on 10.2.177.214:8020
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==token==Kind: HDFS_DELEGATION_TOKEN, Service: 10.2.177.214:8020, Ident: (HDFS_DELEGATION_TOKEN token 16273 for platuser)==
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==cred==org.apache.hadoop.security.Credentials@d90c55c==
16/10/25 11:34:58 INFO SparkHadoopUtil: Stored Hadoop delegation token for user platuser to file file:/home/platuser/spark.token
16/10/25 11:34:58 DEBUG SparkHadoopUtil: ==initDelegationToken().toString==file:/home/platuser/spark.token==
16/10/25 11:34:58 INFO SparkContext: Added JAR file:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-examples-1.6.2-hadoop2.4.1.jar at http://10.2.177.211:53704/jars/spark-examples-1.6.2-hadoop2.4.1.jar with timestamp 1477366498816
16/10/25 11:34:58 INFO AppClient$ClientEndpoint: Connecting to master spark://10.2.177.211:7077...
16/10/25 11:34:58 DEBUG TransportClientFactory: Creating new connection to /10.2.177.211:7077
16/10/25 11:34:58 DEBUG TransportClientFactory: Connection to /10.2.177.211:7077 successful, running bootstraps...
16/10/25 11:34:58 DEBUG TransportClientFactory: Successfully created connection to /10.2.177.211:7077 after 10 ms (0 ms spent in bootstraps)
16/10/25 11:34:59 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20161025113459-0000
16/10/25 11:34:59 DEBUG TransportServer: Shuffle server started on port :58603
16/10/25 11:34:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58603.
16/10/25 11:34:59 INFO NettyBlockTransferService: Server created on 58603
16/10/25 11:34:59 INFO BlockManagerMaster: Trying to register BlockManager
16/10/25 11:34:59 INFO BlockManagerMasterEndpoint: Registering block manager 10.2.177.211:58603 with 511.5 MB RAM, BlockManagerId(driver, 10.2.177.211, 58603)
16/10/25 11:34:59 INFO BlockManagerMaster: Registered BlockManager
16/10/25 11:34:59 INFO AppClient$ClientEndpoint: Executor added: app-20161025113459-0000/0 on worker-20161025113448-10.2.177.211-7078 (10.2.177.211:7078) with 1 cores
16/10/25 11:34:59 INFO SparkDeploySchedulerBackend: Granted executor ID app-20161025113459-0000/0 on hostPort 10.2.177.211:7078 with 1 cores, 1024.0 MB RAM
16/10/25 11:34:59 INFO AppClient$ClientEndpoint: Executor updated: app-20161025113459-0000/0 is now RUNNING
16/10/25 11:34:59 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/10/25 11:34:59 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.examples.SparkPi$$anonfun$1) +++
16/10/25 11:34:59 DEBUG ClosureCleaner:  + declared fields: 1
16/10/25 11:34:59 DEBUG ClosureCleaner:      public static final long org.apache.spark.examples.SparkPi$$anonfun$1.serialVersionUID
16/10/25 11:34:59 DEBUG ClosureCleaner:  + declared methods: 3
16/10/25 11:34:59 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.examples.SparkPi$$anonfun$1.apply(java.lang.Object)
16/10/25 11:34:59 DEBUG ClosureCleaner:      public final int org.apache.spark.examples.SparkPi$$anonfun$1.apply(int)
16/10/25 11:34:59 DEBUG ClosureCleaner:      public int org.apache.spark.examples.SparkPi$$anonfun$1.apply$mcII$sp(int)
16/10/25 11:34:59 DEBUG ClosureCleaner:  + inner classes: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + outer classes: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + outer objects: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/25 11:34:59 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/25 11:34:59 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.examples.SparkPi$$anonfun$1) is now cleaned +++
16/10/25 11:34:59 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.examples.SparkPi$$anonfun$2) +++
16/10/25 11:34:59 DEBUG ClosureCleaner:  + declared fields: 1
16/10/25 11:34:59 DEBUG ClosureCleaner:      public static final long org.apache.spark.examples.SparkPi$$anonfun$2.serialVersionUID
16/10/25 11:34:59 DEBUG ClosureCleaner:  + declared methods: 3
16/10/25 11:34:59 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.examples.SparkPi$$anonfun$2.apply(java.lang.Object,java.lang.Object)
16/10/25 11:34:59 DEBUG ClosureCleaner:      public final int org.apache.spark.examples.SparkPi$$anonfun$2.apply(int,int)
16/10/25 11:34:59 DEBUG ClosureCleaner:      public int org.apache.spark.examples.SparkPi$$anonfun$2.apply$mcIII$sp(int,int)
16/10/25 11:34:59 DEBUG ClosureCleaner:  + inner classes: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + outer classes: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + outer objects: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/25 11:34:59 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/25 11:34:59 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.examples.SparkPi$$anonfun$2) is now cleaned +++
16/10/25 11:34:59 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
16/10/25 11:34:59 DEBUG ClosureCleaner:  + declared fields: 2
16/10/25 11:34:59 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
16/10/25 11:34:59 DEBUG ClosureCleaner:      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
16/10/25 11:34:59 DEBUG ClosureCleaner:  + declared methods: 2
16/10/25 11:34:59 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
16/10/25 11:34:59 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
16/10/25 11:34:59 DEBUG ClosureCleaner:  + inner classes: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + outer classes: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + outer objects: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/25 11:34:59 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/25 11:34:59 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/25 11:34:59 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
16/10/25 11:34:59 INFO SparkContext: Starting job: reduce at SparkPi.scala:36
16/10/25 11:34:59 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:36) with 2 output partitions
16/10/25 11:34:59 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:36)
16/10/25 11:34:59 INFO DAGScheduler: Parents of final stage: List()
16/10/25 11:34:59 INFO DAGScheduler: Missing parents: List()
16/10/25 11:34:59 DEBUG DAGScheduler: submitStage(ResultStage 0)
16/10/25 11:34:59 DEBUG DAGScheduler: missing: List()
16/10/25 11:34:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32), which has no missing parents
16/10/25 11:34:59 DEBUG DAGScheduler: submitMissingTasks(ResultStage 0)
16/10/25 11:35:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1904.0 B, free 1904.0 B)
16/10/25 11:35:00 DEBUG BlockManager: Put block broadcast_0 locally took  123 ms
16/10/25 11:35:00 DEBUG BlockManager: Putting block broadcast_0 without replication took  124 ms
16/10/25 11:35:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1218.0 B, free 3.0 KB)
16/10/25 11:35:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.177.211:58603 (size: 1218.0 B, free: 511.5 MB)
16/10/25 11:35:00 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
16/10/25 11:35:00 DEBUG BlockManager: Told master about block broadcast_0_piece0
16/10/25 11:35:00 DEBUG BlockManager: Put block broadcast_0_piece0 locally took  16 ms
16/10/25 11:35:00 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took  16 ms
16/10/25 11:35:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/10/25 11:35:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32)
16/10/25 11:35:00 DEBUG DAGScheduler: New pending partitions: Set(0, 1)
16/10/25 11:35:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/25 11:35:00 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0
16/10/25 11:35:00 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
16/10/25 11:35:00 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/25 11:35:00 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/25 11:35:01 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/25 11:35:02 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (spark177211:53150) with ID 0
16/10/25 11:35:02 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/25 11:35:02 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
16/10/25 11:35:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, spark177211, partition 0,PROCESS_LOCAL, 2218 bytes)
16/10/25 11:35:02 INFO BlockManagerMasterEndpoint: Registering block manager spark177211:58838 with 511.5 MB RAM, BlockManagerId(0, spark177211, 58838)
16/10/25 11:35:02 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
16/10/25 11:35:03 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
16/10/25 11:35:04 DEBUG BlockManager: Level for block broadcast_0_piece0 is StorageLevel(true, true, false, false, 1)
16/10/25 11:35:04 DEBUG BlockManager: Getting block broadcast_0_piece0 from memory
16/10/25 11:35:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark177211:58838 (size: 1218.0 B, free: 511.5 MB)
16/10/25 11:35:04 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/25 11:35:04 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, spark177211, partition 1,PROCESS_LOCAL, 2218 bytes)
16/10/25 11:35:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2069 ms on spark177211 (1/2)
16/10/25 11:35:04 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
16/10/25 11:35:04 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/25 11:35:04 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
16/10/25 11:35:04 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 86 ms on spark177211 (2/2)
16/10/25 11:35:04 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:36) finished in 4.716 s
16/10/25 11:35:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
16/10/25 11:35:04 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0
16/10/25 11:35:04 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 5.029785 s
16/10/25 11:35:05 INFO SparkUI: Stopped Spark web UI at http://10.2.177.211:4040
16/10/25 11:35:05 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/10/25 11:35:05 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/10/25 11:35:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/25 11:35:05 INFO MemoryStore: MemoryStore cleared
16/10/25 11:35:05 INFO BlockManager: BlockManager stopped
16/10/25 11:35:05 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/25 11:35:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/25 11:35:05 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/10/25 11:35:05 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/10/25 11:35:05 INFO SparkContext: Successfully stopped SparkContext
16/10/25 11:35:05 INFO ShutdownHookManager: Shutdown hook called
16/10/25 11:35:05 INFO ShutdownHookManager: Deleting directory /data_b/spark/spark-96d63621-5512-413d-8426-583d7aa2278e/httpd-8e598449-c0d3-42ae-9bd6-7dccf888f91f
16/10/25 11:35:05 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
16/10/25 11:35:05 INFO ShutdownHookManager: Deleting directory /data_b/spark/spark-96d63621-5512-413d-8426-583d7aa2278e
16/10/25 11:35:05 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@67832d15
16/10/25 11:35:05 DEBUG Client: removing client from cache: org.apache.hadoop.ipc.Client@67832d15
16/10/25 11:35:05 DEBUG Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@67832d15
16/10/25 11:35:05 DEBUG Client: Stopping client
16/10/25 11:35:05 DEBUG Client: IPC Client (1611843139) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: closed
16/10/25 11:35:05 DEBUG Client: IPC Client (1611843139) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: stopped, remaining connections 0