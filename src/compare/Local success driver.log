Launch Command: "/usr/jdk64/jdk1.7.0_45/bin/java" "-cp" "/home/platuser/spark-1.6.2-bin-kerberos/conf/:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-assembly-1.6.2-hadoop2.4.1.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-rdbms-3.2.9.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-core-3.2.10.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/:/etc/hadoop/conf/" "-Xms1024M" "-Xmx1024M" "-Dspark.jars=file:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-examples-1.6.2-hadoop2.4.1.jar" "-Dspark.hadoop.security.kerberos.renewInterval=21600000" "-Dspark.hadoop.security.kerberos.principal=platuser/_HOST@OTOCYON.COM" "-Dspark.submit.deployMode=cluster" "-Dspark.hadoop.security.kerberos.keytab=/home/platuser/platuser.keytab" "-Dspark.app.name=org.apache.spark.examples.SparkPi" "-Dspark.master=spark://10.2.177.211:7077" "-Dspark.hadoop.security.token.name=spark.token" "-Dspark.driver.supervise=true" "-Dspark.hadoop.security.authentication=kerberos" "-Dspark.local.dir=/data_b/spark" "-XX:MaxPermSize=256m" "org.apache.spark.deploy.worker.DriverWrapper" "spark://Worker@10.2.177.211:7078" "/var/run/spark/work/driver-20161024104639-0000/spark-examples-1.6.2-hadoop2.4.1.jar" "org.apache.spark.examples.SparkPi"
========================================

16/10/24 10:46:40 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, about=, value=[Rate of successful kerberos logins and latency (milliseconds)], always=false, type=DEFAULT, sampleName=Ops)
16/10/24 10:46:40 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, about=, value=[Rate of failed kerberos logins and latency (milliseconds)], always=false, type=DEFAULT, sampleName=Ops)
16/10/24 10:46:40 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, about=, value=[GetGroups], always=false, type=DEFAULT, sampleName=Ops)
16/10/24 10:46:40 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
16/10/24 10:46:41 DEBUG Groups:  Creating new Groups object
16/10/24 10:46:41 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
16/10/24 10:46:41 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
16/10/24 10:46:41 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
16/10/24 10:46:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/24 10:46:41 DEBUG JniBasedUnixGroupsMappingWithFallback: Falling back to shell based
16/10/24 10:46:41 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
16/10/24 10:46:41 DEBUG Shell: setsid exited with exit code 0
16/10/24 10:46:41 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
16/10/24 10:46:41 DEBUG UserGroupInformation: hadoop login
16/10/24 10:46:41 DEBUG UserGroupInformation: hadoop login commit
16/10/24 10:46:41 DEBUG UserGroupInformation: using kerberos user:platuser/10.2.177.208@OTOCYON.COM
16/10/24 10:46:41 DEBUG UserGroupInformation: UGI loginUser:platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)
16/10/24 10:46:41 INFO SecurityManager: Changing view acls to: platuser
16/10/24 10:46:41 INFO SecurityManager: Changing modify acls to: platuser
16/10/24 10:46:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(platuser); users with modify permissions: Set(platuser)
16/10/24 10:46:41 DEBUG UserGroupInformation: Found tgt Ticket (hex) =
0000: 61 82 01 78 30 82 01 74   A0 03 02 01 05 A1 0D 1B  a..x0..t........
0010: 0B 4F 54 4F 43 59 4F 4E   2E 43 4F 4D A2 20 30 1E  .OTOCYON.COM. 0.
0020: A0 03 02 01 02 A1 17 30   15 1B 06 6B 72 62 74 67  .......0...krbtg
0030: 74 1B 0B 4F 54 4F 43 59   4F 4E 2E 43 4F 4D A3 82  t..OTOCYON.COM..
0040: 01 3A 30 82 01 36 A0 03   02 01 12 A1 03 02 01 01  .:0..6..........
0050: A2 82 01 28 04 82 01 24   4C FE 74 A5 0C 94 0D FE  ...(...$L.t.....
0060: 2C C9 64 37 E7 E9 E7 46   D4 DB EF 3E 64 BD D0 E8  ,.d7...F...>d...
0070: CA 86 F0 23 07 CE CA CD   62 72 12 DE 75 9D 7A FA  ...#....br..u.z.
0080: 0D A9 A6 DC A4 C8 03 9F   BF 7F 44 DA 57 56 D9 84  ..........D.WV..
0090: B3 6F BD B5 55 78 A4 D4   55 B9 8F 94 C4 DD D1 F5  .o..Ux..U.......
00A0: 73 2B 5E AD B7 BE 4E B6   45 1B DE 4E D1 08 11 2E  s+^...N.E..N....
00B0: 39 FD 6C 22 06 51 0A CE   7F 8D 71 09 72 65 39 99  9.l".Q....q.re9.
00C0: 6D A5 42 1B A6 C9 BE 26   A6 3D 4B 7C 41 7C B4 4F  m.B....&.=K.A..O
00D0: F9 4E 6B 5F 7A A9 63 A5   1E 87 61 47 C0 61 2E BC  .Nk_z.c...aG.a..
00E0: 67 6A 0A 38 D1 F1 1B 18   56 DC CB 2F 5F BD 26 7F  gj.8....V../_.&.
00F0: 7E 58 2E 92 E8 C7 70 F8   43 86 21 C1 E6 BF 07 A4  .X....p.C.!.....
0100: 11 4D 16 D4 C0 3C 5B 99   20 5C 83 66 5A 76 AF 9B  .M...<[. \.fZv..
0110: 6D 59 68 EA 64 9B F1 3B   49 2C 5E BB 2B 86 AF 6C  mYh.d..;I,^.+..l
0120: BE 15 91 79 57 D3 75 CF   EF 4E 43 90 1D EA BF 6B  ...yW.u..NC....k
0130: 6D 25 F5 14 33 48 E5 B6   3C 3E 2B 2B D4 96 AA F8  m%..3H..<>++....
0140: EA 9C E6 43 A6 B0 23 7F   64 9F F8 16 52 E5 1C A2  ...C..#.d...R...
0150: C5 DC 84 A9 83 D8 02 A4   66 74 A5 EA 5E 59 B3 7F  ........ft..^Y..
0160: 76 03 D7 62 77 F5 CD 84   B6 48 44 D0 B6 7D 34 0B  v..bw....HD...4.
0170: FA 5E 71 5F 32 DC F7 EA   93 97 94 D9              .^q_2.......

Client Principal = platuser/10.2.177.208@OTOCYON.COM
Server Principal = krbtgt/OTOCYON.COM@OTOCYON.COM
Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)=
0000: A4 F5 AA 41 83 00 72 10   8D 69 CA 94 6D B0 3F CC  ...A..r..i..m.?.
0010: B0 53 BC 46 CA 07 D9 8C   9C 7F AE D4 60 77 4F CA  .S.F........`wO.


Forwardable Ticket true
Forwarded Ticket false
Proxiable Ticket false
Proxy Ticket false
Postdated Ticket false
Renewable Ticket true
Initial Ticket true
Auth Time = Fri Oct 21 13:22:45 CST 2016
Start Time = Mon Oct 24 10:42:05 CST 2016
End Time = Mon Oct 24 13:22:45 CST 2016
Renew Till = Mon Oct 24 13:22:45 CST 2016
Client Addresses  Null
16/10/24 10:46:41 DEBUG UserGroupInformation: Current time is 1477277201448
16/10/24 10:46:41 DEBUG UserGroupInformation: Next refresh is 1477284637000
16/10/24 10:46:41 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:46:41 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:46:41 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:46:41 DEBUG SecurityManager: SSLConfiguration for file server: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/24 10:46:41 DEBUG SecurityManager: SSLConfiguration for Akka: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/24 10:46:41 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
16/10/24 10:46:41 DEBUG PlatformDependent0: java.nio.Buffer.address: available
16/10/24 10:46:41 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
16/10/24 10:46:41 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
16/10/24 10:46:41 DEBUG PlatformDependent0: java.nio.Bits.unaligned: true
16/10/24 10:46:41 DEBUG PlatformDependent: Java version: 7
16/10/24 10:46:41 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
16/10/24 10:46:41 DEBUG PlatformDependent: sun.misc.Unsafe: available
16/10/24 10:46:41 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
16/10/24 10:46:41 DEBUG PlatformDependent: Javassist: unavailable
16/10/24 10:46:41 DEBUG PlatformDependent: You don't have Javassist in your class path or you don't have enough permission to load dynamically generated classes.  Please check the configuration for better performance.
16/10/24 10:46:41 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
16/10/24 10:46:41 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
16/10/24 10:46:41 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
16/10/24 10:46:41 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
16/10/24 10:46:41 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
16/10/24 10:46:41 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
16/10/24 10:46:41 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
16/10/24 10:46:41 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0x023bcefc642834e0 (took 6 ms)
16/10/24 10:46:42 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
16/10/24 10:46:42 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
16/10/24 10:46:42 DEBUG NetUtil: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)
16/10/24 10:46:42 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
16/10/24 10:46:42 DEBUG TransportServer: Shuffle server started on port :36988
16/10/24 10:46:42 INFO Utils: Successfully started service 'Driver' on port 36988.
16/10/24 10:46:42 INFO WorkerWatcher: Connecting to worker spark://Worker@10.2.177.211:7078
16/10/24 10:46:42 DEBUG TransportClientFactory: Creating new connection to /10.2.177.211:7078
16/10/24 10:46:42 DEBUG ResourceLeakDetector: -Dio.netty.leakDetectionLevel: simple
16/10/24 10:46:42 DEBUG TransportClientFactory: Connection to /10.2.177.211:7078 successful, running bootstraps...
16/10/24 10:46:42 DEBUG TransportClientFactory: Successfully created connection to /10.2.177.211:7078 after 52 ms (0 ms spent in bootstraps)
16/10/24 10:46:42 DEBUG Recycler: -Dio.netty.recycler.maxCapacity.default: 262144
16/10/24 10:46:42 INFO SparkContext: Running Spark version 1.6.2
16/10/24 10:46:42 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
16/10/24 10:46:42 INFO SecurityManager: Changing view acls to: platuser
16/10/24 10:46:42 INFO SecurityManager: Changing modify acls to: platuser
16/10/24 10:46:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(platuser); users with modify permissions: Set(platuser)
16/10/24 10:46:42 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:46:42 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:46:42 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:46:42 DEBUG SecurityManager: SSLConfiguration for file server: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/24 10:46:42 DEBUG SecurityManager: SSLConfiguration for Akka: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/24 10:46:42 DEBUG TransportServer: Shuffle server started on port :36132
16/10/24 10:46:42 INFO Utils: Successfully started service 'sparkDriver' on port 36132.
16/10/24 10:46:42 DEBUG AkkaUtils: In createActorSystem, requireCookie is: off
16/10/24 10:46:42 INFO Slf4jLogger: Slf4jLogger started
16/10/24 10:46:42 INFO Remoting: Starting remoting
16/10/24 10:46:42 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.2.177.211:46858]
16/10/24 10:46:42 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 46858.
16/10/24 10:46:42 DEBUG SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
16/10/24 10:46:42 INFO SparkEnv: Registering MapOutputTracker
16/10/24 10:46:43 INFO SparkEnv: Registering BlockManagerMaster
16/10/24 10:46:43 INFO DiskBlockManager: Created local directory at /data_b/spark/blockmgr-a542f8ad-7b2f-4624-bf60-aa115d6c39f3
16/10/24 10:46:43 INFO MemoryStore: MemoryStore started with capacity 511.5 MB
16/10/24 10:46:43 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/24 10:46:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/24 10:46:43 INFO SparkUI: Started SparkUI at http://10.2.177.211:4040
16/10/24 10:46:43 DEBUG SparkHadoopUtil: ==getAuthenticationType==kerberos==
16/10/24 10:46:43 DEBUG : address: spark177211/10.2.177.211 isLoopbackAddress: false, with host 10.2.177.211 spark177211
16/10/24 10:46:43 DEBUG NativeLibraryLoader: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
16/10/24 10:46:43 DEBUG NativeLibraryLoader: -Dio.netty.netty.workdir: /tmp (io.netty.tmpdir)
16/10/24 10:46:43 DEBUG SparkHadoopUtil: ==localFS==org.apache.hadoop.fs.LocalFileSystem@7b7f8db5==
16/10/24 10:46:43 DEBUG SparkHadoopUtil: ==tokenFile==file:/home/platuser/spark.token==
16/10/24 10:46:43 DEBUG SparkHadoopUtil: ==currentUser==platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)==
16/10/24 10:46:43 DEBUG BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
16/10/24 10:46:43 DEBUG BlockReaderLocal: dfs.client.read.shortcircuit = true
16/10/24 10:46:43 DEBUG BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
16/10/24 10:46:43 DEBUG BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
16/10/24 10:46:43 DEBUG RetryUtils: multipleLinearRandomRetry = null
16/10/24 10:46:43 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@683dd97f
16/10/24 10:46:43 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@579af7c4
16/10/24 10:46:44 WARN BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==fs==DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-100713270_1, ugi=platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)]]==
16/10/24 10:46:44 DEBUG Client: The ping interval is 60000 ms.
16/10/24 10:46:44 DEBUG Client: Connecting to master177214.otocyon.com/10.2.177.214:8020
16/10/24 10:46:44 DEBUG UserGroupInformation: PrivilegedAction as:platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:711)
16/10/24 10:46:44 DEBUG SaslRpcClient: Sending sasl message state: NEGOTIATE

16/10/24 10:46:44 DEBUG SaslRpcClient: Received SASL message state: NEGOTIATE
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
  challenge: "realm=\"default\",nonce=\"le+LAhlqcc75Ci8jHs3Wcu6q2Jm38D32CUQ3mQWZ\",qop=\"auth\",charset=utf-8,algorithm=md5-sess"
}
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "master177214.otocyon.com"
}

16/10/24 10:46:44 DEBUG SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
16/10/24 10:46:44 DEBUG SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=, serverPrincipal=dfs.namenode.kerberos.principal)
16/10/24 10:46:44 DEBUG SaslRpcClient: getting serverKey: dfs.namenode.kerberos.principal conf value: hdfs/_HOST@OTOCYON.COM principal: hdfs/master177214.otocyon.com@OTOCYON.COM
16/10/24 10:46:44 DEBUG SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB is hdfs/master177214.otocyon.com@OTOCYON.COM
16/10/24 10:46:44 DEBUG SaslRpcClient: Creating SASL GSSAPI(KERBEROS)  client to authenticate to service at master177214.otocyon.com
16/10/24 10:46:44 DEBUG SaslRpcClient: Use KERBEROS authentication for protocol ClientNamenodeProtocolPB
16/10/24 10:46:44 DEBUG SaslRpcClient: Sending sasl message state: INITIATE
token: "`\202\002\200\006\t*\206H\206\367\022\001\002\002\001\000n\202\002o0\202\002k\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001ra\202\001n0\202\001j\240\003\002\001\005\241\r\033\vOTOCYON.COM\242+0)\240\003\002\001\000\241\"0 \033\004hdfs\033\030master177214.otocyon.com\243\202\001%0\202\001!\240\003\002\001\022\241\003\002\001\002\242\202\001\023\004\202\001\017\202\345\350\376y\320\245\354\340bt\202\330\324\206\333\b\343\'t\331\336\373c\337*\245\356B\375\350\336\024\2561\203\356\221\363\022NY\370\023\337\221\206\017\211a\260#\220\224\343Xl\034\226\303Q\233\303\223v#\243\220\336`\347u\274\355\211\r%2\212\267\232\\\346\214,\006\204\337\357\333\352z\262\335d\251\362W\016\277\022\313\217\221V\326\211\217\363Ep\376\372\345ih\361\364%J`\026\256\335\312\231t\230\212\200@\364SA\223<\345\214\335\0212\371\322Q\274#\003;0nl\031{|\200\335Z:W\313\'I\021\377\270\361\243\311\355\3246\273\331\273i\254\236\230\260Z\364\340\24629\370\362m\004\212\335\273\215\250\003.$C\000A{\241\307\322\263d,T\247\347F\255\320\323\324K\201\216\207\235\354@\275\005\200$O\2104w*\222kw\316\0338O\234+\016\020\361\201D7jC\356\266i\223J\213T\263\344P\214Z\023\rR\237\272\262\270\262R\270\244\201\3370\201\334\240\003\002\001\022\242\201\324\004\201\321+\205\226\262\017s[\302\253%&p`#\315\343\243\r\3451\370\261\334\037\352B\267\353\241@\227\336_Q5o\016\277\244\237 \221D\366\225\347\032s\021r\351\207\276dKB\206\233#\237\246\376yfh\277\\\bwD\035&\353\255\030\303C\323\303E\323\254\035\364\373<\307N\340P\330#\331\354\375\215S\344LrX\312Tux\365= \0019\232\313\344~\327\0165\205s\370+\3675\307\254\356\314J\377\347]\374!k\002\203\t\034\342\363\306\266\331p\310\017u\006\276\305f1\331/\2377\331\325\216\332z\313D\251\366w;\223\271\036&\377\351 T\243\004 \305G.\035.?\\O\330P\257\206E\353\227\210\21070\252Z\2654]I\332\035\000\205\022"
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "master177214.otocyon.com"
}

16/10/24 10:46:44 DEBUG SaslRpcClient: Received SASL message state: CHALLENGE
token: "`j\006\t*\206H\206\367\022\001\002\002\002\000o[0Y\240\003\002\001\005\241\003\002\001\017\242M0K\240\003\002\001\022\242D\004B\n\"\317;{\272\006\305\263L\037\222\341Q\354\362\265\003\212\213\205K\2436&f\202\271\204/\366TF\002\215\250\326.N}\255\351\222KzN\250\220\316\030K\341\000E;\225A\036\337\221\363\233\343\234\240"

16/10/24 10:46:44 DEBUG SaslRpcClient: Sending sasl message state: RESPONSE
token: ""

16/10/24 10:46:44 DEBUG SaslRpcClient: Received SASL message state: CHALLENGE
token: "\005\004\001\377\000\f\000\000\000\000\000\000\002\026M\343\001\001\000\000i\215\347\035\346\221\203\261a0\225\023"

16/10/24 10:46:44 DEBUG SaslRpcClient: Sending sasl message state: RESPONSE
token: "\005\004\000\377\000\f\000\000\000\000\000\000%H`\273\001\001\000\000Ni\322iGjI\272\245\204+\314"

16/10/24 10:46:44 DEBUG SaslRpcClient: Received SASL message state: SUCCESS

16/10/24 10:46:44 DEBUG Client: Negotiated QOP is :auth
16/10/24 10:46:44 DEBUG Client: IPC Client (1638085305) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: starting, having connections 1
16/10/24 10:46:44 DEBUG Client: IPC Client (1638085305) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM sending #0
16/10/24 10:46:44 DEBUG Client: IPC Client (1638085305) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM got value #0
16/10/24 10:46:44 DEBUG ProtobufRpcEngine: Call: getDelegationToken took 285ms
16/10/24 10:46:44 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 15995 for platuser on 10.2.177.214:8020
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==token==Kind: HDFS_DELEGATION_TOKEN, Service: 10.2.177.214:8020, Ident: (HDFS_DELEGATION_TOKEN token 15995 for platuser)==
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==cred==org.apache.hadoop.security.Credentials@48a6e499==
16/10/24 10:46:44 INFO SparkHadoopUtil: Stored Hadoop delegation token for user platuser to file file:/home/platuser/spark.token
16/10/24 10:46:44 INFO HttpFileServer: HTTP File server directory is /data_b/spark/spark-314f3472-4861-4c5f-9b58-d3b147de901d/httpd-1fbcfd23-c75a-45df-9b08-f98476071d5e
16/10/24 10:46:44 INFO HttpServer: Starting HTTP Server
16/10/24 10:46:44 DEBUG HttpServer: HttpServer is not using security
16/10/24 10:46:44 INFO Utils: Successfully started service 'HTTP file server' on port 37672.
16/10/24 10:46:44 DEBUG HttpFileServer: HTTP file server started at: http://10.2.177.211:37672
16/10/24 10:46:44 INFO Utils: Copying /home/platuser/spark.token to /data_b/spark/spark-314f3472-4861-4c5f-9b58-d3b147de901d/userFiles-5496136c-ddf0-4675-acb4-b07b5c827da2/spark.token
16/10/24 10:46:44 INFO SparkContext: Added file file:/home/platuser/spark.token at http://10.2.177.211:37672/files/spark.token with timestamp 1477277204651
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==localFS==org.apache.hadoop.fs.LocalFileSystem@7b7f8db5==
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==tokenFile==file:/home/platuser/spark.token==
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==currentUser==platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)==
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==fs==DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-100713270_1, ugi=platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)]]==
16/10/24 10:46:44 DEBUG Client: IPC Client (1638085305) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM sending #1
16/10/24 10:46:44 DEBUG Client: IPC Client (1638085305) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM got value #1
16/10/24 10:46:44 DEBUG ProtobufRpcEngine: Call: getDelegationToken took 4ms
16/10/24 10:46:44 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 15996 for platuser on 10.2.177.214:8020
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==token==Kind: HDFS_DELEGATION_TOKEN, Service: 10.2.177.214:8020, Ident: (HDFS_DELEGATION_TOKEN token 15996 for platuser)==
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==cred==org.apache.hadoop.security.Credentials@73922355==
16/10/24 10:46:44 INFO SparkHadoopUtil: Stored Hadoop delegation token for user platuser to file file:/home/platuser/spark.token
16/10/24 10:46:44 DEBUG SparkHadoopUtil: ==initDelegationToken().toString==file:/home/platuser/spark.token==
16/10/24 10:46:44 INFO SparkContext: Added JAR file:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-examples-1.6.2-hadoop2.4.1.jar at http://10.2.177.211:37672/jars/spark-examples-1.6.2-hadoop2.4.1.jar with timestamp 1477277204872
16/10/24 10:46:44 INFO AppClient$ClientEndpoint: Connecting to master spark://10.2.177.211:7077...
16/10/24 10:46:44 DEBUG TransportClientFactory: Creating new connection to /10.2.177.211:7077
16/10/24 10:46:44 DEBUG TransportClientFactory: Connection to /10.2.177.211:7077 successful, running bootstraps...
16/10/24 10:46:44 DEBUG TransportClientFactory: Successfully created connection to /10.2.177.211:7077 after 14 ms (0 ms spent in bootstraps)
16/10/24 10:46:45 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20161024104645-0000
16/10/24 10:46:45 INFO AppClient$ClientEndpoint: Executor added: app-20161024104645-0000/0 on worker-20161024104436-10.2.177.211-7078 (10.2.177.211:7078) with 1 cores
16/10/24 10:46:45 INFO SparkDeploySchedulerBackend: Granted executor ID app-20161024104645-0000/0 on hostPort 10.2.177.211:7078 with 1 cores, 1024.0 MB RAM
16/10/24 10:46:45 DEBUG TransportServer: Shuffle server started on port :38049
16/10/24 10:46:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38049.
16/10/24 10:46:45 INFO NettyBlockTransferService: Server created on 38049
16/10/24 10:46:45 INFO BlockManagerMaster: Trying to register BlockManager
16/10/24 10:46:45 INFO BlockManagerMasterEndpoint: Registering block manager 10.2.177.211:38049 with 511.5 MB RAM, BlockManagerId(driver, 10.2.177.211, 38049)
16/10/24 10:46:45 INFO BlockManagerMaster: Registered BlockManager
16/10/24 10:46:45 INFO AppClient$ClientEndpoint: Executor updated: app-20161024104645-0000/0 is now RUNNING
16/10/24 10:46:45 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/10/24 10:46:45 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.examples.SparkPi$$anonfun$1) +++
16/10/24 10:46:45 DEBUG ClosureCleaner:  + declared fields: 1
16/10/24 10:46:45 DEBUG ClosureCleaner:      public static final long org.apache.spark.examples.SparkPi$$anonfun$1.serialVersionUID
16/10/24 10:46:45 DEBUG ClosureCleaner:  + declared methods: 3
16/10/24 10:46:45 DEBUG ClosureCleaner:      public final int org.apache.spark.examples.SparkPi$$anonfun$1.apply(int)
16/10/24 10:46:45 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.examples.SparkPi$$anonfun$1.apply(java.lang.Object)
16/10/24 10:46:45 DEBUG ClosureCleaner:      public int org.apache.spark.examples.SparkPi$$anonfun$1.apply$mcII$sp(int)
16/10/24 10:46:45 DEBUG ClosureCleaner:  + inner classes: 0
16/10/24 10:46:45 DEBUG ClosureCleaner:  + outer classes: 0
16/10/24 10:46:45 DEBUG ClosureCleaner:  + outer objects: 0
16/10/24 10:46:45 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/24 10:46:45 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/24 10:46:45 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/24 10:46:45 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.examples.SparkPi$$anonfun$1) is now cleaned +++
16/10/24 10:46:46 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.examples.SparkPi$$anonfun$2) +++
16/10/24 10:46:46 DEBUG ClosureCleaner:  + declared fields: 1
16/10/24 10:46:46 DEBUG ClosureCleaner:      public static final long org.apache.spark.examples.SparkPi$$anonfun$2.serialVersionUID
16/10/24 10:46:46 DEBUG ClosureCleaner:  + declared methods: 3
16/10/24 10:46:46 DEBUG ClosureCleaner:      public final int org.apache.spark.examples.SparkPi$$anonfun$2.apply(int,int)
16/10/24 10:46:46 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.examples.SparkPi$$anonfun$2.apply(java.lang.Object,java.lang.Object)
16/10/24 10:46:46 DEBUG ClosureCleaner:      public int org.apache.spark.examples.SparkPi$$anonfun$2.apply$mcIII$sp(int,int)
16/10/24 10:46:46 DEBUG ClosureCleaner:  + inner classes: 0
16/10/24 10:46:46 DEBUG ClosureCleaner:  + outer classes: 0
16/10/24 10:46:46 DEBUG ClosureCleaner:  + outer objects: 0
16/10/24 10:46:46 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/24 10:46:46 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/24 10:46:46 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/24 10:46:46 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.examples.SparkPi$$anonfun$2) is now cleaned +++
16/10/24 10:46:46 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
16/10/24 10:46:46 DEBUG ClosureCleaner:  + declared fields: 2
16/10/24 10:46:46 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
16/10/24 10:46:46 DEBUG ClosureCleaner:      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
16/10/24 10:46:46 DEBUG ClosureCleaner:  + declared methods: 2
16/10/24 10:46:46 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
16/10/24 10:46:46 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
16/10/24 10:46:46 DEBUG ClosureCleaner:  + inner classes: 0
16/10/24 10:46:46 DEBUG ClosureCleaner:  + outer classes: 0
16/10/24 10:46:46 DEBUG ClosureCleaner:  + outer objects: 0
16/10/24 10:46:46 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/24 10:46:46 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/24 10:46:46 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/24 10:46:46 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
16/10/24 10:46:46 INFO SparkContext: Starting job: reduce at SparkPi.scala:36
16/10/24 10:46:46 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:36) with 2 output partitions
16/10/24 10:46:46 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:36)
16/10/24 10:46:46 INFO DAGScheduler: Parents of final stage: List()
16/10/24 10:46:46 INFO DAGScheduler: Missing parents: List()
16/10/24 10:46:46 DEBUG DAGScheduler: submitStage(ResultStage 0)
16/10/24 10:46:46 DEBUG DAGScheduler: missing: List()
16/10/24 10:46:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32), which has no missing parents
16/10/24 10:46:46 DEBUG DAGScheduler: submitMissingTasks(ResultStage 0)
16/10/24 10:46:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1904.0 B, free 1904.0 B)
16/10/24 10:46:46 DEBUG BlockManager: Put block broadcast_0 locally took  166 ms
16/10/24 10:46:46 DEBUG BlockManager: Putting block broadcast_0 without replication took  167 ms
16/10/24 10:46:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1218.0 B, free 3.0 KB)
16/10/24 10:46:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.177.211:38049 (size: 1218.0 B, free: 511.5 MB)
16/10/24 10:46:46 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
16/10/24 10:46:46 DEBUG BlockManager: Told master about block broadcast_0_piece0
16/10/24 10:46:46 DEBUG BlockManager: Put block broadcast_0_piece0 locally took  12 ms
16/10/24 10:46:46 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took  13 ms
16/10/24 10:46:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/10/24 10:46:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32)
16/10/24 10:46:46 DEBUG DAGScheduler: New pending partitions: Set(0, 1)
16/10/24 10:46:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/24 10:46:46 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0
16/10/24 10:46:46 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
16/10/24 10:46:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/24 10:46:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/24 10:46:47 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/24 10:46:48 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (spark177211:34277) with ID 0
16/10/24 10:46:48 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/24 10:46:48 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
16/10/24 10:46:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, spark177211, partition 0,PROCESS_LOCAL, 2218 bytes)
16/10/24 10:46:48 INFO BlockManagerMasterEndpoint: Registering block manager spark177211:38062 with 511.5 MB RAM, BlockManagerId(0, spark177211, 38062)
16/10/24 10:46:48 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
16/10/24 10:46:49 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 1
16/10/24 10:46:50 DEBUG BlockManager: Level for block broadcast_0_piece0 is StorageLevel(true, true, false, false, 1)
16/10/24 10:46:50 DEBUG BlockManager: Getting block broadcast_0_piece0 from memory
16/10/24 10:46:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark177211:38062 (size: 1218.0 B, free: 511.5 MB)
16/10/24 10:46:50 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/24 10:46:50 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, spark177211, partition 1,PROCESS_LOCAL, 2218 bytes)
16/10/24 10:46:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1899 ms on spark177211 (1/2)
16/10/24 10:46:50 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
16/10/24 10:46:50 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
16/10/24 10:46:50 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:36) finished in 4.320 s
16/10/24 10:46:50 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0
16/10/24 10:46:50 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 66 ms on spark177211 (2/2)
16/10/24 10:46:50 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 4.694003 s
16/10/24 10:46:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
16/10/24 10:46:50 INFO SparkUI: Stopped Spark web UI at http://10.2.177.211:4040
16/10/24 10:46:50 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/10/24 10:46:50 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/10/24 10:46:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/24 10:46:50 INFO MemoryStore: MemoryStore cleared
16/10/24 10:46:50 INFO BlockManager: BlockManager stopped
16/10/24 10:46:50 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/24 10:46:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/24 10:46:50 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/10/24 10:46:50 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/10/24 10:46:50 INFO SparkContext: Successfully stopped SparkContext
16/10/24 10:46:50 INFO ShutdownHookManager: Shutdown hook called
16/10/24 10:46:50 INFO ShutdownHookManager: Deleting directory /data_b/spark/spark-314f3472-4861-4c5f-9b58-d3b147de901d/httpd-1fbcfd23-c75a-45df-9b08-f98476071d5e
16/10/24 10:46:50 INFO ShutdownHookManager: Deleting directory /data_b/spark/spark-314f3472-4861-4c5f-9b58-d3b147de901d
16/10/24 10:46:50 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
16/10/24 10:46:50 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@579af7c4
16/10/24 10:46:50 DEBUG Client: removing client from cache: org.apache.hadoop.ipc.Client@579af7c4
16/10/24 10:46:50 DEBUG Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@579af7c4
16/10/24 10:46:50 DEBUG Client: Stopping client
16/10/24 10:46:50 DEBUG Client: IPC Client (1638085305) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: closed
16/10/24 10:46:50 DEBUG Client: IPC Client (1638085305) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: stopped, remaining connections 0