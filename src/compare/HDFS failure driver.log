Launch Command: "/usr/jdk64/jdk1.7.0_45/bin/java" "-cp" "/home/platuser/spark-1.6.2-bin-kerberos/conf/:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-assembly-1.6.2-hadoop2.4.1.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-rdbms-3.2.9.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-core-3.2.10.jar:/home/platuser/spark-1.6.2-bin-kerberos/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/:/etc/hadoop/conf/" "-Xms1024M" "-Xmx1024M" "-Dspark.jars=file:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-examples-1.6.2-hadoop2.4.1.jar" "-Dspark.hadoop.security.kerberos.renewInterval=21600000" "-Dspark.hadoop.security.kerberos.principal=platuser/_HOST@OTOCYON.COM" "-Dspark.submit.deployMode=cluster" "-Dspark.hadoop.security.kerberos.keytab=/home/platuser/platuser.keytab" "-Dspark.master=spark://10.2.177.211:7077" "-Dspark.app.name=org.apache.spark.examples.HdfsTest" "-Dspark.hadoop.security.token.name=spark.token" "-Dspark.driver.supervise=true" "-Dspark.hadoop.security.authentication=kerberos" "-Dspark.local.dir=/data_b/spark" "-XX:MaxPermSize=256m" "org.apache.spark.deploy.worker.DriverWrapper" "spark://Worker@10.2.177.211:7078" "/var/run/spark/work/driver-20161024104722-0001/spark-examples-1.6.2-hadoop2.4.1.jar" "org.apache.spark.examples.HdfsTest" "/user/platuser/test2.txt"
========================================

16/10/24 10:47:23 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)], about=, type=DEFAULT, always=false, sampleName=Ops)
16/10/24 10:47:23 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)], about=, type=DEFAULT, always=false, sampleName=Ops)
16/10/24 10:47:23 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[GetGroups], about=, type=DEFAULT, always=false, sampleName=Ops)
16/10/24 10:47:23 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
16/10/24 10:47:23 DEBUG Groups:  Creating new Groups object
16/10/24 10:47:23 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
16/10/24 10:47:23 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
16/10/24 10:47:23 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
16/10/24 10:47:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/24 10:47:23 DEBUG JniBasedUnixGroupsMappingWithFallback: Falling back to shell based
16/10/24 10:47:23 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
16/10/24 10:47:23 DEBUG Shell: setsid exited with exit code 0
16/10/24 10:47:23 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
16/10/24 10:47:23 DEBUG UserGroupInformation: hadoop login
16/10/24 10:47:23 DEBUG UserGroupInformation: hadoop login commit
16/10/24 10:47:23 DEBUG UserGroupInformation: using kerberos user:platuser/10.2.177.208@OTOCYON.COM
16/10/24 10:47:23 DEBUG UserGroupInformation: UGI loginUser:platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)
16/10/24 10:47:23 INFO SecurityManager: Changing view acls to: platuser
16/10/24 10:47:23 INFO SecurityManager: Changing modify acls to: platuser
16/10/24 10:47:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(platuser); users with modify permissions: Set(platuser)
16/10/24 10:47:23 DEBUG UserGroupInformation: Found tgt Ticket (hex) =
0000: 61 82 01 78 30 82 01 74   A0 03 02 01 05 A1 0D 1B  a..x0..t........
0010: 0B 4F 54 4F 43 59 4F 4E   2E 43 4F 4D A2 20 30 1E  .OTOCYON.COM. 0.
0020: A0 03 02 01 02 A1 17 30   15 1B 06 6B 72 62 74 67  .......0...krbtg
0030: 74 1B 0B 4F 54 4F 43 59   4F 4E 2E 43 4F 4D A3 82  t..OTOCYON.COM..
0040: 01 3A 30 82 01 36 A0 03   02 01 12 A1 03 02 01 01  .:0..6..........
0050: A2 82 01 28 04 82 01 24   4C FE 74 A5 0C 94 0D FE  ...(...$L.t.....
0060: 2C C9 64 37 E7 E9 E7 46   D4 DB EF 3E 64 BD D0 E8  ,.d7...F...>d...
0070: CA 86 F0 23 07 CE CA CD   62 72 12 DE 75 9D 7A FA  ...#....br..u.z.
0080: 0D A9 A6 DC A4 C8 03 9F   BF 7F 44 DA 57 56 D9 84  ..........D.WV..
0090: B3 6F BD B5 55 78 A4 D4   55 B9 8F 94 C4 DD D1 F5  .o..Ux..U.......
00A0: 73 2B 5E AD B7 BE 4E B6   45 1B DE 4E D1 08 11 2E  s+^...N.E..N....
00B0: 39 FD 6C 22 06 51 0A CE   7F 8D 71 09 72 65 39 99  9.l".Q....q.re9.
00C0: 6D A5 42 1B A6 C9 BE 26   A6 3D 4B 7C 41 7C B4 4F  m.B....&.=K.A..O
00D0: F9 4E 6B 5F 7A A9 63 A5   1E 87 61 47 C0 61 2E BC  .Nk_z.c...aG.a..
00E0: 67 6A 0A 38 D1 F1 1B 18   56 DC CB 2F 5F BD 26 7F  gj.8....V../_.&.
00F0: 7E 58 2E 92 E8 C7 70 F8   43 86 21 C1 E6 BF 07 A4  .X....p.C.!.....
0100: 11 4D 16 D4 C0 3C 5B 99   20 5C 83 66 5A 76 AF 9B  .M...<[. \.fZv..
0110: 6D 59 68 EA 64 9B F1 3B   49 2C 5E BB 2B 86 AF 6C  mYh.d..;I,^.+..l
0120: BE 15 91 79 57 D3 75 CF   EF 4E 43 90 1D EA BF 6B  ...yW.u..NC....k
0130: 6D 25 F5 14 33 48 E5 B6   3C 3E 2B 2B D4 96 AA F8  m%..3H..<>++....
0140: EA 9C E6 43 A6 B0 23 7F   64 9F F8 16 52 E5 1C A2  ...C..#.d...R...
0150: C5 DC 84 A9 83 D8 02 A4   66 74 A5 EA 5E 59 B3 7F  ........ft..^Y..
0160: 76 03 D7 62 77 F5 CD 84   B6 48 44 D0 B6 7D 34 0B  v..bw....HD...4.
0170: FA 5E 71 5F 32 DC F7 EA   93 97 94 D9              .^q_2.......

Client Principal = platuser/10.2.177.208@OTOCYON.COM
Server Principal = krbtgt/OTOCYON.COM@OTOCYON.COM
Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)=
0000: A4 F5 AA 41 83 00 72 10   8D 69 CA 94 6D B0 3F CC  ...A..r..i..m.?.
0010: B0 53 BC 46 CA 07 D9 8C   9C 7F AE D4 60 77 4F CA  .S.F........`wO.


Forwardable Ticket true
Forwarded Ticket false
Proxiable Ticket false
Proxy Ticket false
Postdated Ticket false
Renewable Ticket true
Initial Ticket true
Auth Time = Fri Oct 21 13:22:45 CST 2016
Start Time = Mon Oct 24 10:42:05 CST 2016
End Time = Mon Oct 24 13:22:45 CST 2016
Renew Till = Mon Oct 24 13:22:45 CST 2016
Client Addresses  Null
16/10/24 10:47:23 DEBUG UserGroupInformation: Current time is 1477277243898
16/10/24 10:47:23 DEBUG UserGroupInformation: Next refresh is 1477284637000
16/10/24 10:47:23 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:47:24 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:47:24 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:47:24 DEBUG SecurityManager: SSLConfiguration for file server: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/24 10:47:24 DEBUG SecurityManager: SSLConfiguration for Akka: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/24 10:47:24 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
16/10/24 10:47:24 DEBUG PlatformDependent0: java.nio.Buffer.address: available
16/10/24 10:47:24 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
16/10/24 10:47:24 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
16/10/24 10:47:24 DEBUG PlatformDependent0: java.nio.Bits.unaligned: true
16/10/24 10:47:24 DEBUG PlatformDependent: Java version: 7
16/10/24 10:47:24 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
16/10/24 10:47:24 DEBUG PlatformDependent: sun.misc.Unsafe: available
16/10/24 10:47:24 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
16/10/24 10:47:24 DEBUG PlatformDependent: Javassist: unavailable
16/10/24 10:47:24 DEBUG PlatformDependent: You don't have Javassist in your class path or you don't have enough permission to load dynamically generated classes.  Please check the configuration for better performance.
16/10/24 10:47:24 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
16/10/24 10:47:24 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
16/10/24 10:47:24 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
16/10/24 10:47:24 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
16/10/24 10:47:24 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
16/10/24 10:47:24 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
16/10/24 10:47:24 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
16/10/24 10:47:24 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0x2b4bb436c9295894 (took 6 ms)
16/10/24 10:47:24 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
16/10/24 10:47:24 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
16/10/24 10:47:24 DEBUG NetUtil: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)
16/10/24 10:47:24 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128
16/10/24 10:47:24 DEBUG TransportServer: Shuffle server started on port :38853
16/10/24 10:47:24 INFO Utils: Successfully started service 'Driver' on port 38853.
16/10/24 10:47:24 INFO WorkerWatcher: Connecting to worker spark://Worker@10.2.177.211:7078
16/10/24 10:47:24 DEBUG TransportClientFactory: Creating new connection to /10.2.177.211:7078
16/10/24 10:47:24 DEBUG ResourceLeakDetector: -Dio.netty.leakDetectionLevel: simple
16/10/24 10:47:24 DEBUG TransportClientFactory: Connection to /10.2.177.211:7078 successful, running bootstraps...
16/10/24 10:47:24 DEBUG TransportClientFactory: Successfully created connection to /10.2.177.211:7078 after 54 ms (0 ms spent in bootstraps)
16/10/24 10:47:24 DEBUG Recycler: -Dio.netty.recycler.maxCapacity.default: 262144
16/10/24 10:47:24 INFO SparkContext: Running Spark version 1.6.2
16/10/24 10:47:24 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
16/10/24 10:47:24 INFO SecurityManager: Changing view acls to: platuser
16/10/24 10:47:24 INFO SecurityManager: Changing modify acls to: platuser
16/10/24 10:47:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(platuser); users with modify permissions: Set(platuser)
16/10/24 10:47:24 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:47:24 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:47:24 DEBUG SSLOptions: No SSL protocol specified
16/10/24 10:47:24 DEBUG SecurityManager: SSLConfiguration for file server: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/24 10:47:24 DEBUG SecurityManager: SSLConfiguration for Akka: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
16/10/24 10:47:24 DEBUG TransportServer: Shuffle server started on port :58881
16/10/24 10:47:24 INFO Utils: Successfully started service 'sparkDriver' on port 58881.
16/10/24 10:47:24 DEBUG AkkaUtils: In createActorSystem, requireCookie is: off
16/10/24 10:47:25 INFO Slf4jLogger: Slf4jLogger started
16/10/24 10:47:25 INFO Remoting: Starting remoting
16/10/24 10:47:25 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.2.177.211:37526]
16/10/24 10:47:25 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 37526.
16/10/24 10:47:25 DEBUG SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
16/10/24 10:47:25 INFO SparkEnv: Registering MapOutputTracker
16/10/24 10:47:25 INFO SparkEnv: Registering BlockManagerMaster
16/10/24 10:47:25 INFO DiskBlockManager: Created local directory at /data_b/spark/blockmgr-4565c2ec-bb92-4b10-bf95-f398d922f0f1
16/10/24 10:47:25 INFO MemoryStore: MemoryStore started with capacity 511.5 MB
16/10/24 10:47:25 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/24 10:47:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/24 10:47:25 INFO SparkUI: Started SparkUI at http://10.2.177.211:4040
16/10/24 10:47:26 DEBUG SparkHadoopUtil: ==getAuthenticationType==kerberos==
16/10/24 10:47:26 DEBUG : address: spark177211/10.2.177.211 isLoopbackAddress: false, with host 10.2.177.211 spark177211
16/10/24 10:47:26 DEBUG NativeLibraryLoader: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
16/10/24 10:47:26 DEBUG NativeLibraryLoader: -Dio.netty.netty.workdir: /tmp (io.netty.tmpdir)
16/10/24 10:47:26 DEBUG SparkHadoopUtil: ==localFS==org.apache.hadoop.fs.LocalFileSystem@6e5e53d==
16/10/24 10:47:26 DEBUG SparkHadoopUtil: ==tokenFile==file:/home/platuser/spark.token==
16/10/24 10:47:26 DEBUG SparkHadoopUtil: ==currentUser==platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)==
16/10/24 10:47:26 DEBUG BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
16/10/24 10:47:26 DEBUG BlockReaderLocal: dfs.client.read.shortcircuit = true
16/10/24 10:47:26 DEBUG BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
16/10/24 10:47:26 DEBUG BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
16/10/24 10:47:26 DEBUG RetryUtils: multipleLinearRandomRetry = null
16/10/24 10:47:26 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@7360b926
16/10/24 10:47:26 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@7e63a5e9
16/10/24 10:47:26 WARN BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
16/10/24 10:47:26 DEBUG SparkHadoopUtil: ==fs==DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1385033022_1, ugi=platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)]]==
16/10/24 10:47:26 DEBUG Client: The ping interval is 60000 ms.
16/10/24 10:47:26 DEBUG Client: Connecting to master177214.otocyon.com/10.2.177.214:8020
16/10/24 10:47:26 DEBUG UserGroupInformation: PrivilegedAction as:platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:711)
16/10/24 10:47:26 DEBUG SaslRpcClient: Sending sasl message state: NEGOTIATE

16/10/24 10:47:26 DEBUG SaslRpcClient: Received SASL message state: NEGOTIATE
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
  challenge: "realm=\"default\",nonce=\"rTla9x3Aiuwfl0lqMGSKwVf4E2/QYu9cuZMI86bF\",qop=\"auth\",charset=utf-8,algorithm=md5-sess"
}
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "master177214.otocyon.com"
}

16/10/24 10:47:26 DEBUG SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
16/10/24 10:47:26 DEBUG SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=, serverPrincipal=dfs.namenode.kerberos.principal)
16/10/24 10:47:26 DEBUG SaslRpcClient: getting serverKey: dfs.namenode.kerberos.principal conf value: hdfs/_HOST@OTOCYON.COM principal: hdfs/master177214.otocyon.com@OTOCYON.COM
16/10/24 10:47:26 DEBUG SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB is hdfs/master177214.otocyon.com@OTOCYON.COM
16/10/24 10:47:26 DEBUG SaslRpcClient: Creating SASL GSSAPI(KERBEROS)  client to authenticate to service at master177214.otocyon.com
16/10/24 10:47:26 DEBUG SaslRpcClient: Use KERBEROS authentication for protocol ClientNamenodeProtocolPB
16/10/24 10:47:26 DEBUG SaslRpcClient: Sending sasl message state: INITIATE
token: "`\202\002\200\006\t*\206H\206\367\022\001\002\002\001\000n\202\002o0\202\002k\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001ra\202\001n0\202\001j\240\003\002\001\005\241\r\033\vOTOCYON.COM\242+0)\240\003\002\001\000\241\"0 \033\004hdfs\033\030master177214.otocyon.com\243\202\001%0\202\001!\240\003\002\001\022\241\003\002\001\002\242\202\001\023\004\202\001\017\303Y\230fAM\245\375\006\224\337Y\301A\366\232\353c\261\346\212#\334\224g\336\f\023\000\351)\026\362\371\237\2670\342\274U\235}NJ\260zU\265\234\331\'\'\202\323N\3153\375\263SA\002o\203\337\'\035\025\346\330\246#\244\303w\360\324\377\221\220(\v\314wz\205\217;;\261x{ \210\224\371\3053\377\234c\035\310\031&\243\203\371\215f\235\016\347\325\330c\240\a\352\343\234&\t\255\2269\216\20650\244\031P\021\210\020uC\257#\r\262\312F\016dH\367\332\320\263\232\325\301\221\034\v\003\342\270F\325\355\216\371\002\037>\366b\253\023\361\223F\2033\030v\3108\303\001\317W\370\326\325\331\323~\'\242bg\004&\200=\bO2\260\267zo\351n\223\362\244\032\203H(\001\206\r|H\024aw\304\270jhy\337|C\362g\021P\220Q!E))+\205o\275`XN!\270\231%(\221\363[\000\353b\233\317s2\321\"z}(\322\bM\244\201\3370\201\334\240\003\002\001\022\242\201\324\004\201\321\305$\231\016,\245\3479r\204\341\031\f\263\034\232\265\325(\006n\212\003V-=u\362\227\324\235 \006\\Ht\310\210*\276\016mo\313<\334~\254\336\000\b\234kI\210\323\321\365P\305\332\\\340\344\254FQ\334\240\374\355\033wBV\021^ks\277Z\f\257\024\r\313\355,\327k\\\363I\211\006\217b\357a;C\b]\257\207\201\314\f\342\220!w6\235\234%\366lj\321T~\351\020\274\271^F\036\312I\317H\325\0023b\314\210\370\253\300\252\222\253q\v)\374$\317\262\362f\005\201L\236\330\210\203\364\220\372\236\022w(\323\372#H\245\257TY]\305j!A\242H\317h4N\033\025\373d\005U\306Z>D\005sP\27440(\247\321n\320\226"
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "master177214.otocyon.com"
}

16/10/24 10:47:26 DEBUG SaslRpcClient: Received SASL message state: CHALLENGE
token: "`j\006\t*\206H\206\367\022\001\002\002\002\000o[0Y\240\003\002\001\005\241\003\002\001\017\242M0K\240\003\002\001\022\242D\004BD\313h\fd(I \311\245\350\264PX\324\36603!,7E\017\366\267F\034\345\2569\345\020\"\035&\3648\367\b\364\200\306\265\025p\276\347A\266ja\205\237.\360z\307\256\030)\236\fuK\231\244"

16/10/24 10:47:26 DEBUG SaslRpcClient: Sending sasl message state: RESPONSE
token: ""

16/10/24 10:47:26 DEBUG SaslRpcClient: Received SASL message state: CHALLENGE
token: "\005\004\001\377\000\f\000\000\000\000\000\000\000\250\205\324\001\001\000\000|A\002\a\021\244\203}\215_\b\251"

16/10/24 10:47:26 DEBUG SaslRpcClient: Sending sasl message state: RESPONSE
token: "\005\004\000\377\000\f\000\000\000\000\000\0004\004i\267\001\001\000\000\027;\346Y\252\205v\211{\365\366O"

16/10/24 10:47:26 DEBUG SaslRpcClient: Received SASL message state: SUCCESS

16/10/24 10:47:26 DEBUG Client: Negotiated QOP is :auth
16/10/24 10:47:26 DEBUG Client: IPC Client (1382781624) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: starting, having connections 1
16/10/24 10:47:26 DEBUG Client: IPC Client (1382781624) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM sending #0
16/10/24 10:47:26 DEBUG Client: IPC Client (1382781624) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM got value #0
16/10/24 10:47:26 DEBUG ProtobufRpcEngine: Call: getDelegationToken took 276ms
16/10/24 10:47:26 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 15997 for platuser on 10.2.177.214:8020
16/10/24 10:47:27 DEBUG SparkHadoopUtil: ==token==Kind: HDFS_DELEGATION_TOKEN, Service: 10.2.177.214:8020, Ident: (HDFS_DELEGATION_TOKEN token 15997 for platuser)==
16/10/24 10:47:27 DEBUG SparkHadoopUtil: ==cred==org.apache.hadoop.security.Credentials@10ef0242==
16/10/24 10:47:27 INFO SparkHadoopUtil: Stored Hadoop delegation token for user platuser to file file:/home/platuser/spark.token
16/10/24 10:47:27 INFO HttpFileServer: HTTP File server directory is /data_b/spark/spark-93ce62bb-e1fa-46d4-b8c8-03c8e85c283f/httpd-783e8485-44ea-4eab-9c40-2fa38ca471d6
16/10/24 10:47:27 INFO HttpServer: Starting HTTP Server
16/10/24 10:47:27 DEBUG HttpServer: HttpServer is not using security
16/10/24 10:47:27 INFO Utils: Successfully started service 'HTTP file server' on port 60354.
16/10/24 10:47:27 DEBUG HttpFileServer: HTTP file server started at: http://10.2.177.211:60354
16/10/24 10:47:27 INFO Utils: Copying /home/platuser/spark.token to /data_b/spark/spark-93ce62bb-e1fa-46d4-b8c8-03c8e85c283f/userFiles-6e77a0b1-6464-4ce7-bafb-0bfd23a446cc/spark.token
16/10/24 10:47:27 INFO SparkContext: Added file file:/home/platuser/spark.token at http://10.2.177.211:60354/files/spark.token with timestamp 1477277247087
16/10/24 10:47:27 DEBUG SparkHadoopUtil: ==localFS==org.apache.hadoop.fs.LocalFileSystem@6e5e53d==
16/10/24 10:47:27 DEBUG SparkHadoopUtil: ==tokenFile==file:/home/platuser/spark.token==
16/10/24 10:47:27 DEBUG SparkHadoopUtil: ==currentUser==platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)==
16/10/24 10:47:27 DEBUG SparkHadoopUtil: ==fs==DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1385033022_1, ugi=platuser/10.2.177.208@OTOCYON.COM (auth:KERBEROS)]]==
16/10/24 10:47:27 DEBUG Client: IPC Client (1382781624) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM sending #1
16/10/24 10:47:27 DEBUG Client: IPC Client (1382781624) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM got value #1
16/10/24 10:47:27 DEBUG ProtobufRpcEngine: Call: getDelegationToken took 3ms
16/10/24 10:47:27 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 15998 for platuser on 10.2.177.214:8020
16/10/24 10:47:27 DEBUG SparkHadoopUtil: ==token==Kind: HDFS_DELEGATION_TOKEN, Service: 10.2.177.214:8020, Ident: (HDFS_DELEGATION_TOKEN token 15998 for platuser)==
16/10/24 10:47:27 DEBUG SparkHadoopUtil: ==cred==org.apache.hadoop.security.Credentials@3689f87c==
16/10/24 10:47:27 INFO SparkHadoopUtil: Stored Hadoop delegation token for user platuser to file file:/home/platuser/spark.token
16/10/24 10:47:27 DEBUG SparkHadoopUtil: ==initDelegationToken().toString==file:/home/platuser/spark.token==
16/10/24 10:47:27 INFO SparkContext: Added JAR file:/home/platuser/spark-1.6.2-bin-kerberos/lib/spark-examples-1.6.2-hadoop2.4.1.jar at http://10.2.177.211:60354/jars/spark-examples-1.6.2-hadoop2.4.1.jar with timestamp 1477277247302
16/10/24 10:47:27 INFO AppClient$ClientEndpoint: Connecting to master spark://10.2.177.211:7077...
16/10/24 10:47:27 DEBUG TransportClientFactory: Creating new connection to /10.2.177.211:7077
16/10/24 10:47:27 DEBUG TransportClientFactory: Connection to /10.2.177.211:7077 successful, running bootstraps...
16/10/24 10:47:27 DEBUG TransportClientFactory: Successfully created connection to /10.2.177.211:7077 after 19 ms (0 ms spent in bootstraps)
16/10/24 10:47:27 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20161024104727-0001
16/10/24 10:47:27 INFO AppClient$ClientEndpoint: Executor added: app-20161024104727-0001/0 on worker-20161024104436-10.2.177.211-7078 (10.2.177.211:7078) with 1 cores
16/10/24 10:47:27 INFO SparkDeploySchedulerBackend: Granted executor ID app-20161024104727-0001/0 on hostPort 10.2.177.211:7078 with 1 cores, 1024.0 MB RAM
16/10/24 10:47:27 DEBUG TransportServer: Shuffle server started on port :48913
16/10/24 10:47:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 48913.
16/10/24 10:47:27 INFO NettyBlockTransferService: Server created on 48913
16/10/24 10:47:27 INFO BlockManagerMaster: Trying to register BlockManager
16/10/24 10:47:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.2.177.211:48913 with 511.5 MB RAM, BlockManagerId(driver, 10.2.177.211, 48913)
16/10/24 10:47:27 INFO BlockManagerMaster: Registered BlockManager
16/10/24 10:47:27 INFO AppClient$ClientEndpoint: Executor updated: app-20161024104727-0001/0 is now RUNNING
16/10/24 10:47:27 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/10/24 10:47:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 223.0 KB, free 223.0 KB)
16/10/24 10:47:28 DEBUG BlockManager: Put block broadcast_0 locally took  192 ms
16/10/24 10:47:28 DEBUG BlockManager: Putting block broadcast_0 without replication took  193 ms
16/10/24 10:47:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.8 KB, free 242.9 KB)
16/10/24 10:47:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.177.211:48913 (size: 19.8 KB, free: 511.5 MB)
16/10/24 10:47:28 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
16/10/24 10:47:28 DEBUG BlockManager: Told master about block broadcast_0_piece0
16/10/24 10:47:28 DEBUG BlockManager: Put block broadcast_0_piece0 locally took  7 ms
16/10/24 10:47:28 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took  7 ms
16/10/24 10:47:28 INFO SparkContext: Created broadcast 0 from textFile at HdfsTest.scala:34
16/10/24 10:47:28 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33) +++
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared fields: 2
16/10/24 10:47:28 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.serialVersionUID
16/10/24 10:47:28 DEBUG ClosureCleaner:      private final org.apache.spark.SparkContext$$anonfun$hadoopFile$1 org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.$outer
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared methods: 2
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final void org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(org.apache.hadoop.mapred.JobConf)
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33.apply(java.lang.Object)
16/10/24 10:47:28 DEBUG ClosureCleaner:  + inner classes: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer classes: 2
16/10/24 10:47:28 DEBUG ClosureCleaner:      org.apache.spark.SparkContext$$anonfun$hadoopFile$1
16/10/24 10:47:28 DEBUG ClosureCleaner:      org.apache.spark.SparkContext
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer objects: 2
16/10/24 10:47:28 DEBUG ClosureCleaner:      <function0>
16/10/24 10:47:28 DEBUG ClosureCleaner:      org.apache.spark.SparkContext@717b11a9
16/10/24 10:47:28 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/24 10:47:28 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
16/10/24 10:47:28 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext,Set())
16/10/24 10:47:28 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext$$anonfun$hadoopFile$1,Set(path$6))
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outermost object is not a closure, so do not clone it: (class org.apache.spark.SparkContext,org.apache.spark.SparkContext@717b11a9)
16/10/24 10:47:28 DEBUG ClosureCleaner:  + cloning the object <function0> of class org.apache.spark.SparkContext$$anonfun$hadoopFile$1
16/10/24 10:47:28 DEBUG ClosureCleaner:  + cleaning cloned closure <function0> recursively (org.apache.spark.SparkContext$$anonfun$hadoopFile$1)
16/10/24 10:47:28 DEBUG ClosureCleaner: +++ Cleaning closure <function0> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1) +++
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared fields: 7
16/10/24 10:47:28 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$hadoopFile$1.serialVersionUID
16/10/24 10:47:28 DEBUG ClosureCleaner:      private final org.apache.spark.SparkContext org.apache.spark.SparkContext$$anonfun$hadoopFile$1.$outer
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final java.lang.String org.apache.spark.SparkContext$$anonfun$hadoopFile$1.path$6
16/10/24 10:47:28 DEBUG ClosureCleaner:      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.inputFormatClass$1
16/10/24 10:47:28 DEBUG ClosureCleaner:      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.keyClass$1
16/10/24 10:47:28 DEBUG ClosureCleaner:      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.valueClass$1
16/10/24 10:47:28 DEBUG ClosureCleaner:      private final int org.apache.spark.SparkContext$$anonfun$hadoopFile$1.minPartitions$3
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared methods: 2
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final org.apache.spark.rdd.HadoopRDD org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply()
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply()
16/10/24 10:47:28 DEBUG ClosureCleaner:  + inner classes: 1
16/10/24 10:47:28 DEBUG ClosureCleaner:      org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer classes: 1
16/10/24 10:47:28 DEBUG ClosureCleaner:      org.apache.spark.SparkContext
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer objects: 1
16/10/24 10:47:28 DEBUG ClosureCleaner:      org.apache.spark.SparkContext@717b11a9
16/10/24 10:47:28 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
16/10/24 10:47:28 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext,Set())
16/10/24 10:47:28 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext$$anonfun$hadoopFile$1,Set(path$6))
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outermost object is not a closure, so do not clone it: (class org.apache.spark.SparkContext,org.apache.spark.SparkContext@717b11a9)
16/10/24 10:47:28 DEBUG ClosureCleaner:  + the starting closure doesn't actually need org.apache.spark.SparkContext@717b11a9, so we null it out
16/10/24 10:47:28 DEBUG ClosureCleaner:  +++ closure <function0> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1) is now cleaned +++
16/10/24 10:47:28 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$33) is now cleaned +++
16/10/24 10:47:28 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9) +++
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared fields: 1
16/10/24 10:47:28 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9.serialVersionUID
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared methods: 2
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final java.lang.String org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9.apply(scala.Tuple2)
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9.apply(java.lang.Object)
16/10/24 10:47:28 DEBUG ClosureCleaner:  + inner classes: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer classes: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer objects: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/24 10:47:28 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/24 10:47:28 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$9) is now cleaned +++
16/10/24 10:47:28 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.examples.HdfsTest$$anonfun$1) +++
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared fields: 1
16/10/24 10:47:28 DEBUG ClosureCleaner:      public static final long org.apache.spark.examples.HdfsTest$$anonfun$1.serialVersionUID
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared methods: 2
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final int org.apache.spark.examples.HdfsTest$$anonfun$1.apply(java.lang.String)
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.examples.HdfsTest$$anonfun$1.apply(java.lang.Object)
16/10/24 10:47:28 DEBUG ClosureCleaner:  + inner classes: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer classes: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer objects: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/24 10:47:28 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/24 10:47:28 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.examples.HdfsTest$$anonfun$1) is now cleaned +++
16/10/24 10:47:28 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1) +++
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared fields: 1
16/10/24 10:47:28 DEBUG ClosureCleaner:      public static final long org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.serialVersionUID
16/10/24 10:47:28 DEBUG ClosureCleaner:  + declared methods: 3
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(java.lang.Object)
16/10/24 10:47:28 DEBUG ClosureCleaner:      public final void org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(int)
16/10/24 10:47:28 DEBUG ClosureCleaner:      public void org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(int)
16/10/24 10:47:28 DEBUG ClosureCleaner:  + inner classes: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer classes: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + outer objects: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
16/10/24 10:47:28 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/10/24 10:47:28 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/10/24 10:47:28 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.examples.HdfsTest$$anonfun$main$1$$anonfun$apply$mcVI$sp$1) is now cleaned +++
16/10/24 10:47:28 DEBUG BlockManager: Getting local block broadcast_0
16/10/24 10:47:28 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(true, true, false, true, 1)
16/10/24 10:47:28 DEBUG BlockManager: Getting block broadcast_0 from memory
16/10/24 10:47:28 DEBUG HadoopRDD: SplitLocationInfo and other new Hadoop classes are unavailable. Using the older Hadoop location info code.
java.lang.ClassNotFoundException: org.apache.hadoop.mapred.InputSplitWithLocationInfo
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.rdd.HadoopRDD$SplitInfoReflections.<init>(HadoopRDD.scala:392)
	at org.apache.spark.rdd.HadoopRDD$.liftedTree1$1(HadoopRDD.scala:402)
	at org.apache.spark.rdd.HadoopRDD$.<init>(HadoopRDD.scala:401)
	at org.apache.spark.rdd.HadoopRDD$.<clinit>(HadoopRDD.scala)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:165)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:195)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1935)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:912)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:910)
	at org.apache.spark.examples.HdfsTest$$anonfun$main$1.apply$mcVI$sp(HdfsTest.scala:38)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.examples.HdfsTest$.main(HdfsTest.scala:36)
	at org.apache.spark.examples.HdfsTest.main(HdfsTest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)
16/10/24 10:47:28 DEBUG HadoopRDD: Creating new JobConf and caching it for later re-use
Exception in thread "main" java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)
Caused by: java.io.IOException: Can't get Master Kerberos principal for use as renewer
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:116)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:304)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1935)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:912)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:910)
	at org.apache.spark.examples.HdfsTest$$anonfun$main$1.apply$mcVI$sp(HdfsTest.scala:38)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.examples.HdfsTest$.main(HdfsTest.scala:36)
	at org.apache.spark.examples.HdfsTest.main(HdfsTest.scala)
	... 6 more
16/10/24 10:47:28 INFO SparkContext: Invoking stop() from shutdown hook
16/10/24 10:47:28 INFO SparkUI: Stopped Spark web UI at http://10.2.177.211:4040
16/10/24 10:47:28 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/10/24 10:47:28 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/10/24 10:47:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/24 10:47:28 INFO MemoryStore: MemoryStore cleared
16/10/24 10:47:28 INFO BlockManager: BlockManager stopped
16/10/24 10:47:28 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/24 10:47:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/24 10:47:28 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/10/24 10:47:28 INFO SparkContext: Successfully stopped SparkContext
16/10/24 10:47:28 INFO ShutdownHookManager: Shutdown hook called
16/10/24 10:47:28 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/10/24 10:47:28 INFO ShutdownHookManager: Deleting directory /data_b/spark/spark-93ce62bb-e1fa-46d4-b8c8-03c8e85c283f/httpd-783e8485-44ea-4eab-9c40-2fa38ca471d6
16/10/24 10:47:28 INFO ShutdownHookManager: Deleting directory /data_b/spark/spark-93ce62bb-e1fa-46d4-b8c8-03c8e85c283f
16/10/24 10:47:28 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@7e63a5e9
16/10/24 10:47:28 DEBUG Client: removing client from cache: org.apache.hadoop.ipc.Client@7e63a5e9
16/10/24 10:47:28 DEBUG Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@7e63a5e9
16/10/24 10:47:28 DEBUG Client: Stopping client
16/10/24 10:47:28 DEBUG Client: IPC Client (1382781624) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: closed
16/10/24 10:47:28 DEBUG Client: IPC Client (1382781624) connection to master177214.otocyon.com/10.2.177.214:8020 from platuser/10.2.177.208@OTOCYON.COM: stopped, remaining connections 0
16/10/24 10:47:28 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.